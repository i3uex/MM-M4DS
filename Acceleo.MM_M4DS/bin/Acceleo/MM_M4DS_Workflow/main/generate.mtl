[comment encoding = UTF-8 /]
[**
 * The documentation of the module generate.
 */]
[module generate('https://www.example.org/workflow', 'https://www.example.org/contract', 'http://www.example.org/Library', 'https://www.example.org/environment')]

[import Acceleo::MM_M4DS_contract::main::generate/]


[**
 * The documentation of the template generateElement.
 * @param aWorkflow
 */]

[template public generateWorkflow(aWorkflow : Workflow)]
[comment @main/]
[generateEnvironment(aWorkflow)/]
[fileFormatting(aWorkflow)/]
[pmmlProcessing(aWorkflow)/]
[generateDataProcessing(aWorkflow)/]
[generateContractsDataProcessing(aWorkflow)/]
[generateTransformationsDataProcessing(aWorkflow)/]
[/template]


[template public generateDataProcessing(aWorkflow : Workflow)]
[file ('dataProcessing_Job_'+aWorkflow.name.replaceAll('[(),\\s]+', '_')+'.py', false, 'UTF-8')]
import pandas as pd
import numpy as np
import functions.contract_invariants as contract_invariants
import functions.contract_pre_post as contract_pre_post
import functions.data_transformations as data_transformations
from helpers.enumerations import Belong, Operator, Operation, SpecialType, DataType, DerivedType, Closure, FilterType
from helpers.logger import set_logger
import pyarrow

[comment generate the imports and the class/]
def generateWorkflow():
[for (ps : ProcessingStep | aWorkflow.dataprocessing)]																										[comment OPEN traverse all the DataProcessings/]
	[if (ps.incoming = null and ps.outgoing=null)]																											[comment OPEN checks if the DataProcessing is isolated (no links)/]
		[if (ps.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]																	[comment OPEN checks if the dataPrcessingDefinition is of type DataProcessingDefinition/]
			[let dpd : Library::DataProcessingDefinition = ps.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]						[comment OPEN assigns the variable to be of type DataProcessingDefinition/]
				[if (dpd.oclIsKindOf(Library::Transformation))]																								[comment OPEN checks if the dataPrcessingDefinition is of type Transformation/]
					[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]															[comment OPEN assigns the variable to be of type Transformation/]
	#-----------------New DataProcessing-----------------
						[for (dc : DataDictionary | ps.inputPort)]																							[comment OPEN read all the input datasets and store them/]
							[if (aWorkflow.environment.oclIsKindOf(Local))]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[for (st : Storage | ps.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0 and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][/if][/for][if (dc.fileName.trim().indexOf('.')>0)][dc.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dc.fileName.trim()/].parquet[/if]')
							[/if]

						[/for]																																[comment CLOSE read all the input datasets and store them/]
						[if (ps.oclIsKindOf(DataProcessing))]
							[let d : DataProcessing = ps.oclAsType(DataProcessing)]
								[for (c : Contract | d.contract)]																							[comment OPEN traverse all the contracts in the DataProcessing/]
									[if (c.contract.type=ContractType::PRECONDITION)]																		[comment OPEN checks if the if the contract is a Precondition/]
	[generateCallContract(c.contract, d)/]
									[/if]																													[comment CLOSE checks if the if the contract is a Precondition/]
								[/for]																														[comment CLOSE traverse all the contracts in the DataProcessing/]
	[generateCallTransformation(d, d)/]
								[for (c : Contract | d.contract)]																							[comment OPEN traverse all the contracts in the DataProcessing/]
									[if (c.contract.type=ContractType::POSTCONDITION)]																		[comment OPEN checks if the if the contract is a Postcondition/]
	[generateCallContract(c.contract, d)/]
									[/if]																													[comment CLOSE checks if the if the contract is a Postcondition/]
								[/for]																														[comment CLOSE traverse all the contracts in the DataProcessing/]
								[for (c : Contract | d.contract)]																							[comment OPEN traverse all the contracts in the DataProcessing/]
									[if (c.contract.type=ContractType::INVARIANT)]																			[comment OPEN checks if the if the contract is a Invariant/]
	[generateCallContract(c.contract, d)/]
									[/if]																													[comment CLOSE checks if the if the contract is a Invariant/]
								[/for]																														[comment CLOSE traverse all the contracts in the DataProcessing/]
							[/let]
						[/if]
						[if (ps.oclIsKindOf(PMMLModel))]
							[let pmml : PMMLModel = ps.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
							[/let]
						[/if]
					[/let]																																	[comment CLOSE assigns the variable to be of type Transformation/]
				[elseif (dpd.oclIsKindOf(Library::Job))]																									[comment OPEN checks if the dataPrcessingDefinition is of type Job/]
					[let libJ : Library::Job = dpd.oclAsType(Library::Job)]																					[comment OPEN assigns the variable to be of type Job/]
	#--------------------------------------Input data dictionaries--------------------------------------
						[for (arg : Argument | ps._in)]																										[comment OPEN traverse all the input arguments/]
							[if (arg.oclIsKindOf(DataDictionary))]																							[comment OPEN checks if the argument is a DataDictionary/]
								[let dd_in : DataDictionary = arg.oclAsType(DataDictionary)]																[comment OPEN assigns the variables to be a DataDictionary/]
									[if (aWorkflow.environment.oclIsKindOf(Local))]																			[comment OPEN checks if it is a local file and a container/]
	[dd_in.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[for (st : Storage | ps.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.fileName.trim().indexOf('.')>0)][dd_in.fileName.replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.fileName.trim()/].parquet[/if]'
									[/if]																													[comment CLOSE checks if it is a local file and a container or a local environmenr/]
								[/let]																														[comment CLOSE assigns the variables to be a DataDictionary/]
							[/if]																															[comment CLOSE checks if the argument is a DataDictionary/]
						[/for]																																[comment CLOSE traverse all the input arguments/]
	#--------------------------------------Output data dictionaries--------------------------------------
						[for (arg : Argument | ps.out)]																										[comment OPEN traverse all the output arguments/]
							[if (arg.oclIsKindOf(DataDictionary))]																							[comment OPEN checks if the argument is a DataDictionary/]
								[let dd_out : DataDictionary = arg.oclAsType(DataDictionary)]																[comment OPEN assigns the variables to be a DataDictionary/]
									[if (aWorkflow.environment.oclIsKindOf(Local))]																			[comment OPEN checks if it is a local file and a container/]
	[dd_out.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[for (st : Storage | ps.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_out.fileName.trim().indexOf('.')>0)][dd_out.fileName.replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_out.fileName.trim()/].parquet[/if]'
									[/if]																													[comment CLOSE checks if it is a local file and a container or a local environment/]
								[/let]																														[comment CLOSE assigns the variables to be a DataDictionary/]
							[/if]																															[comment CLOSE checks if the argument is a DataDictionary/]
						[/for]																																[comment CLOSE traverse all the output arguments/]

						[for (pstep : ProcessingStep | libJ.workflow.dataprocessing)]																		[comment OPEN traverse the dataProcessings in the workflow/]
							[if (pstep.incoming = null and pstep.outgoing=null)]																			[comment OPEN checks if the dataProcessing is isolated/]
								[if (pstep.oclIsKindOf(DataProcessing))]
									[let dataP : DataProcessing = pstep.oclAsType(DataProcessing)]
										[if (ps.oclIsKindOf(DataProcessing))]
											[let d : DataProcessing = ps.oclAsType(DataProcessing)]
												[for (c : Contract | dataP.contract)]																		[comment OPEN traverse all the contracts in the DataProcessing/]
													[if (c.contract.type=ContractType::PRECONDITION)]														[comment OPEN checks if the if the contract is a Precondition/]
	[generateCallContract(c.contract, d)/]
													[/if]																									[comment CLOSE checks if the if the contract is a Precondition/]
												[/for]																										[comment CLOSE traverse all the contracts in the DataProcessing/]
	[generateCallTransformation(dataP, d)/]
												[for (c : Contract | dataP.contract)]																		[comment OPEN traverse all the contracts in the DataProcessing/]
													[if (c.contract.type=ContractType::POSTCONDITION)]														[comment OPEN checks if the if the contract is a Postcondition/]
	[generateCallContract(c.contract, d)/]
													[/if]																									[comment CLOSE checks if the if the contract is a Postcondition/]
												[/for]																										[comment CLOSE traverse all the contracts in the DataProcessing/]
												[for (c : Contract | dataP.contract)]																		[comment OPEN traverse all the contracts in the DataProcessing/]
													[if (c.contract.type=ContractType::INVARIANT)]															[comment OPEN checks if the if the contract is a Invariant/]
	[generateCallContract(c.contract, d)/]
													[/if]																									[comment CLOSE checks if the if the contract is a Invariant/]
												[/for]																										[comment CLOSE traverse all the contracts in the DataProcessing/]
											[/let]
										[/if]
									[/let]
								[/if]
								[if (ps.oclIsKindOf(DataProcessing))]
									[let pmml : PMMLModel = pstep.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
									[/let]
								[/if]
							[/if]																															[comment CLOSE checks if the dataProcessing is isolated/]
						[/for]																																[comment CLOSE traverse the dataProcessings in the workflow/]
						[for (pstep : ProcessingStep | libJ.workflow.dataprocessing)]																		[comment OPEN traverse the dataProcessings in the workflow/]
							[if (pstep.incoming = null and pstep.outgoing<>null)]																			[comment OPEN checks if the dataProcessing is isolated/]
								[if (pstep.oclIsKindOf(DataProcessing))]
									[let dataP : DataProcessing = pstep.oclAsType(DataProcessing)]
										[if (ps.oclIsKindOf(DataProcessing))]
											[let d : DataProcessing = ps.oclAsType(DataProcessing)]
[callModifiedRecursiveTemplate(dataP, d)/]
											[/let]
										[/if]
									[/let]
								[/if]
								[if (pstep.oclIsKindOf(PMMLModel))]
									[let pmml : PMMLModel = pstep.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
									[/let]
								[/if]
							[/if]																															[comment CLOSE checks that the dataProcessing is not isolated and is the first/]
						[/for]																																[comment CLOSE traverse the dataProcessings in the workflow/]
					[/let]																																	[comment CLOSE assigns the variable to be of type Job/]
				[/if]																																		[comment CLOSE checks if the dataPrcessingDefinition is of type Transformation or Job/]
			[/let]																																			[comment CLOSE assigns the variable to be of type DataProcessingDefinition/]
		[/if]																																				[comment CLOSE checks if the dataPrcessingDefinition is of type DataProcessingDefinition/]
	[/if]																																					[comment CLOSE checks if the DataProcessing is isolated (no links)/]
[/for]																																						[comment CLOSE traverse all the DataProcessings/]

[for (ps : ProcessingStep | aWorkflow.dataprocessing)]																										[comment OPEN traverse all the DataProcessings/]
	[if (ps.incoming = null and ps.outgoing<>null)]																											[comment OPEN checks if the DataProcessing is the first (no input link, an output link)/]
		[if (ps.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]																	[comment OPEN checks if the dataPrcessingDefinition is of type DataProcessingDefinition/]
			[let dpd : Library::DataProcessingDefinition = ps.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]						[comment OPEN assigns the variable to be of type DataProcessingDefinition/]
				[if (dpd.oclIsKindOf(Library::Transformation))]																								[comment OPEN checks if the dataPrcessingDefinition is of type Transformation/]
					[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]															[comment OPEN assigns the variable to be of type Transformation/]
						[if (ps.oclIsKindOf(DataProcessing))]
							[let d : DataProcessing = ps.oclAsType(DataProcessing)]
[callRecursiveTemplate(d)/]
							[/let]
						[/if]
						[if (ps.oclIsKindOf(PMMLModel))]
							[let pmml : PMMLModel = ps.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
							[/let]
						[/if]
					[/let]																																	[comment CLOSE assigns the variable to be of type Transformation/]
				[elseif (dpd.oclIsKindOf(Library::Job))]																									[comment OPEN checks if the dataPrcessingDefinition is of type Job/]
					[let libJ : Library::Job = dpd.oclAsType(Library::Job)]																					[comment OPEN assigns the variable to be of type Job/]
	#--------------------------------------Input data dictionaries--------------------------------------
						[for (arg : Argument | ps._in)]																										[comment OPEN traverse all the input arguments/]
							[if (arg.oclIsKindOf(DataDictionary))]																							[comment OPEN checks if the argument is a DataDictionary/]
								[let dd_in : DataDictionary = arg.oclAsType(DataDictionary)]																[comment OPEN assigns the variables to be a DataDictionary/]
									[if (aWorkflow.environment.oclIsKindOf(Local))]																			[comment OPEN checks if it is a local file and a container/]
	[dd_in.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[for (st : Storage | ps.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.fileName.trim().indexOf('.')>0)][dd_in.fileName.replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.fileName.trim()/].parquet[/if]'
									[/if]																													[comment CLOSE checks if it is a local file and container or a local environment/]
								[/let]																														[comment CLOSE assigns the variables to be a DataDictionary/]
							[/if]																															[comment CLOSE checks if the argument is a DataDictionary/]
						[/for]																																[comment CLOSE traverse all the input arguments/]
	#--------------------------------------Output data dictionaries--------------------------------------
						[for (arg : Argument | ps.out)]																										[comment OPEN traverse all the output arguments/]
							[if (arg.oclIsKindOf(DataDictionary))]																							[comment OPEN checks if the argument is a DataDictionary/]
								[let dd_out : DataDictionary = arg.oclAsType(DataDictionary)]																[comment OPEN assigns the variables to be a DataDictionary/]
									[if (aWorkflow.environment.oclIsKindOf(Local))]																			[comment OPEN checks if it is a local file and a container/]
	[dd_out.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[for (st : Storage | ps.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_out.fileName.trim().indexOf('.')>0)][dd_out.fileName.replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_out.fileName.trim()/].parquet[/if]'
									[/if]																													[comment CLOSE checks if it is a local file and container or a local environment/]
								[/let]																														[comment CLOSE assigns the variables to be a DataDictionary/]
							[/if]																															[comment CLOSE checks if the argument is a DataDictionary/]
						[/for]																																[comment CLOSE traverse all the output arguments/]
						
						[if (ps.oclIsKindOf(DataProcessing))]
							[let d : DataProcessing = ps.oclAsType(DataProcessing)]
								[for (pstep : ProcessingStep | libJ.workflow.dataprocessing)]																[comment OPEN traverse the dataProcessings in the workflow/]
									[if (pstep.incoming = null and pstep.outgoing=null)]																	[comment OPEN checks if the dataProcessing is isolated/]
										[if (pstep.oclIsKindOf(DataProcessing))]
											[let dataP : DataProcessing = pstep.oclAsType(DataProcessing)]
												[for (c : Contract | dataP.contract)]																		[comment OPEN traverse all the contracts in the DataProcessing/]
													[if (c.contract.type=ContractType::PRECONDITION)]														[comment OPEN checks if the if the contract is a Precondition/]
	[generateCallContract(c.contract, d)/]
													[/if]																									[comment CLOSE checks if the if the contract is a Precondition/]
												[/for]																										[comment CLOSE traverse all the contracts in the DataProcessing/]
	[generateCallTransformation(dataP, d)/]
												[for (c : Contract | dataP.contract)]																		[comment OPEN traverse all the contracts in the DataProcessing/]
													[if (c.contract.type=ContractType::POSTCONDITION)]														[comment OPEN checks if the if the contract is a Postcondition/]
	[generateCallContract(c.contract, d)/]
													[/if]																									[comment CLOSE checks if the if the contract is a Postcondition/]
												[/for]																										[comment CLOSE traverse all the contracts in the DataProcessing/]
												[for (c : Contract | dataP.contract)]																		[comment OPEN traverse all the contracts in the DataProcessing/]
													[if (c.contract.type=ContractType::INVARIANT)]															[comment OPEN checks if the if the contract is a Invariant/]
	[generateCallContract(c.contract, d)/]
													[/if]																									[comment CLOSE checks if the if the contract is a Invariant/]
												[/for]																										[comment CLOSE traverse all the contracts in the DataProcessing/]
											[/let]
										[/if]
									[/if]																													[comment CLOSE checks if the dataProcessing is isolated/]
									[if (pstep.oclIsKindOf(PMMLModel))]
										[let pmml : PMMLModel = pstep.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
										[/let]
									[/if]
								[/for]																														[comment CLOSE traverse the dataProcessings in the workflow/]
								[for (pstep : ProcessingStep | libJ.workflow.dataprocessing)]																[comment OPEN traverse the dataProcessings in the workflow/]
									[if (pstep.incoming = null and pstep.outgoing<>null)]																	[comment OPEN checks if the dataProcessing is isolated/]
										[if (pstep.oclIsKindOf(DataProcessing))]
											[let dataP : DataProcessing = pstep.oclAsType(DataProcessing)]													[comment OPEN checks that the dataProcessing is not isolated and is the first/]
[callModifiedRecursiveTemplate(dataP, d)/]
											[/let]
										[/if]		
										[if (pstep.oclIsKindOf(PMMLModel))]
											[let pmml : PMMLModel = pstep.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
											[/let]
										[/if]
									[/if]																													[comment CLOSE checks that the dataProcessing is not isolated and is the first/]
								[/for]																														[comment CLOSE traverse the dataProcessings in the workflow/]
							[/let]
						[/if]
						[if (ps.oclIsKindOf(PMMLModel))]
							[let pmml : PMMLModel = ps.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
							[/let]
						[/if]
					[/let]																																	[comment CLOSE assigns the variable to be of type Job/]
				[/if]																																		[comment CLOSE checks if the dataPrcessingDefinition is of type Transformation or Job/]
			[/let]																																			[comment CLOSE assigns the variable to be of type DataProcessingDefinition/]
		[/if]																																				[comment CLOSE checks if the dataPrcessingDefinition is of type DataProcessingDefinition/]
	[/if]																																					[comment CLOSE checks if the DataProcessing is the first (no input link, an output link)/]
[/for]																																						[comment CLOSE traverse all the DataProcessings/]
set_logger("dataProcessing")
generateWorkflow()
[/file]
[/template]


[template public callModifiedRecursiveTemplate(dw : DataProcessing, d_or : DataProcessing)]
	#-----------------New DataProcessing-----------------
[if (dw.incoming <> null)]															[comment OPEN checks that it is not the first data processing/]
	[if (dw.incoming.source.dataProcessingDefinition.name<>'split')]				[comment OPEN checks if the operation is not split/]
	[dw.inputPort->first().name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet([dw.inputPort->first().name.replaceAll('[(),\\s]+', '_')/])

	[else]																			[comment OPEN checks that the operation is split/]
		[for (dc : DataDictionary | dw.inputPort)]									[comment OPEN read all the input datasets and store them/]
			[for (dc_out : DataDictionary | dw.incoming.source.outputPort)]			[comment OPEN read the previous DataProcessing output datasets/]
				[if (dc.fileName.trim() = dc_out.fileName)]								[comment OPEN checks that both datasets are the same/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet([dc.name.replaceAll('[(),\\s]+', '_')/])
				[/if]																[comment CLOSE checks that both datasets are the same/]
			[/for]																	[comment CLOSE read the previous DataProcessing output datasets/]
		[/for]																		[comment CLOSE read all the input datasets and store them/]
	[/if]																			[comment CLOSE checks whether the operation is split or not/]
[else]																				[comment OPEN checks if it is the first data processing/]
	[for (dc : DataDictionary | dw.inputPort)]										[comment OPEN read all the input datasets and store them/]
		[if (d_or.eContainer(Workflow).environment.oclIsKindOf(Local))]				[comment OPEN checks if it is a local file and a container/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet([dc.name.replaceAll('[(),\\s]+', '_')/])
		[/if]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet([dc.name.replaceAll('[(),\\s]+', '_')/])

	[/for]																			[comment CLOSE read all the input datasets and store them/]
[/if]																				[comment CLOSE checks whether it is the first data processing or not/]
[for (c : Contract | dw.contract)]													[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::PRECONDITION)]								[comment OPEN checks if the if the contract is a Precondition/]
	[generateCallContract(c.contract, d_or)/]
	[/if]																			[comment CLOSE checks if the if the contract is a Precondition/]
[/for]																				[comment CLOSE traverse all the contracts in the DataProcessing/]
	[generateCallTransformation(dw, d_or)/]	
[for (c : Contract | dw.contract)]													[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::POSTCONDITION)]								[comment OPEN checks if the if the contract is a Postcondition/]
	[generateCallContract(c.contract, d_or)/]
	[/if]																			[comment CLOSE checks if the if the contract is a Postcondition/]
[/for]																				[comment CLOSE traverse all the contracts in the DataProcessing/]
[for (c : Contract | dw.contract)]													[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::INVARIANT)]									[comment OPEN checks if the if the contract is a Invariant/]
	[generateCallContract(c.contract, d_or)/]
	[/if]																			[comment CLOSE checks if the if the contract is a Invariant/]
[/for]																				[comment CLOSE traverse all the contracts in the DataProcessing/]
[if (dw.outgoing<>null)]															[comment OPEN checks if the DataProcessing has an outgoing link (it's not the last)/]
	[let nextDp : DataProcessing = dw.outgoing.target.oclAsType(DataProcessing)]	[comment OPEN assigns the next DataProcessing to a variable/]
[callModifiedRecursiveTemplate(nextDp, d_or)/]			[comment Recursive call to this template to generate all the DataProcessings linked to the first/]
	[/let]																			[comment OPEN assigns the next DataProcessing to a variable/]
[/if]																				[comment CLOSE checks if the DataProcessing has an outgoing link (it's not the last)/]
[/template]


[template public callRecursiveTemplate(d : DataProcessing)]
	#-----------------New DataProcessing-----------------
[if (d.incoming <> null)]																								[comment OPEN checks that it is not the first data processing/]
	[if (d.incoming.source.dataProcessingDefinition.name<>'split')]														[comment OPEN checks that the operation is not split/]
		[if (d.eContainer(Workflow).environment.oclIsKindOf(Local))]													[comment OPEN checks if it is a local file and a container/]
	[d.inputPort->first().name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | d.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=d.inputPort->first())][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=d.inputPort->first())][st.storageName/]/[/if][/for][/let][/if][/for][if (d.inputPort->first().fileName.trim().indexOf('.')>0)][d.inputPort->first().fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][d.inputPort->first().fileName.trim()/].parquet[/if]')
		[/if]																											[comment CLOSE checks if it is a local file and a container or a local environment/]

	[else]																												[comment OPEN checks if the operation is split/]
		[for (dc : DataDictionary | d.inputPort)]																		[comment OPEN read all the input datasets and store them/]
			[for (dc_out : DataDictionary | d.incoming.source.outputPort)]												[comment OPEN read the previous DataProcessing output datasets/]
				[if (dc.fileName.trim() = dc_out.fileName)]																[comment OPEN checks that both datasets are the same/]
					[if (d.eContainer(Workflow).environment.oclIsKindOf(Local))]										[comment OPEN checks if it is a local file and a container/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | d.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dc_out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dc_out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dc_out.fileName.trim().indexOf('.')>0)][dc_out.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dc_out.fileName.trim()/].parquet[/if]')
					[/if]																								[comment CLOSE checks if it is a local file and a container or a local environment/]
				[/if]																									[comment CLOSE checks that both datasets are the same/]
			[/for]																										[comment CLOSE read the previous DataProcessing output datasets/]
		[/for]																											[comment CLOSE read all the input datasets and store them/]
	[/if]																												[comment CLOSE checks whether the operation is split or not/]
[else]																													[comment CLOSE checks that it is not the first data processing/]
	[for (dc : DataDictionary | d.inputPort)]																			[comment OPEN read all the input datasets and store them/]
		[if (d.eContainer(Workflow).environment.oclIsKindOf(Local))]													[comment OPEN checks if it is a local file and a container/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | d.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][/if][/for][if (dc.fileName.trim().indexOf('.')>0)][dc.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dc.fileName.trim()/].parquet[/if]')
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | d.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][/if][/for][if (dc.fileName.trim().indexOf('.')>0)][dc.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dc.fileName.trim()/].parquet[/if]')
		[/if]																											[comment CLOSE checks if it is a local file and a container or a local environment/]

	[/for]																												[comment CLOSE read all the input datasets and store them/]
[/if]																													[comment CLOSE checks whether it is the first data processing or not/]
[for (c : Contract | d.contract)]																						[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::PRECONDITION)]																	[comment OPEN checks if the if the contract is a Precondition/]
	[generateCallContract(c.contract, d)/]
	[/if]																												[comment CLOSE checks if the if the contract is a Precondition/]
[/for]																													[comment CLOSE traverse all the contracts in the DataProcessing/]
	[generateCallTransformation(d, d)/]
[for (c : Contract | d.contract)]																						[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::POSTCONDITION)]																	[comment OPEN checks if the if the contract is a Postcondition/]
	[generateCallContract(c.contract, d)/]
	[/if]																												[comment CLOSE checks if the if the contract is a Postcondition/]
[/for]																													[comment CLOSE traverse all the contracts in the DataProcessing/]
[for (c : Contract | d.contract)]																						[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::INVARIANT)]																		[comment OPEN checks if the if the contract is a Invariant/]
	[generateCallContract(c.contract, d)/]
	[/if]																												[comment CLOSE checks if the if the contract is a Invariant/]
[/for]																													[comment CLOSE traverse all the contracts in the DataProcessing/]
[if (d.outgoing<>null)]																									[comment OPEN checks if the DataProcessing has an outgoing link (it's not the last)/]
	[let nextDp : DataProcessing = d.outgoing.target.oclAsType(DataProcessing)]											[comment OPEN assigns the next DataProcessing to a variable/]
[callRecursiveTemplate(nextDp)/]							[comment Recursive call to this template to generate all the DataProcessings linked to the first/]
	[/let]																												[comment OPEN assigns the next DataProcessing to a variable/]
[/if]																													[comment CLOSE checks if the DataProcessing has an outgoing link (it's not the last)/]
[/template]


[template public generateContractsDataProcessing(aWorkflow : Workflow)]
[file ('contracts_Job_'+aWorkflow.name.replaceAll('[(),\\s]+', '_')+'.py', false, 'UTF-8')]
import os

import pandas as pd
import numpy as np
import functions.contract_invariants as contract_invariants
import functions.contract_pre_post as contract_pre_post
from helpers.enumerations import Belong, Operator, Operation, SpecialType, DataType, DerivedType, Closure, FilterType
from helpers.logger import set_logger
import pyarrow

[comment generate the imports and the class/]
def generateWorkflow():
[for (ps : ProcessingStep | aWorkflow.dataprocessing)]																										[comment OPEN traverse all the DataProcessings/]
	[if (ps.incoming = null and ps.outgoing=null)]																											[comment OPEN checks if the DataProcessing is isolated (no links)/]
	#-----------------New DataProcessing-----------------
		[if (ps.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]																	[comment OPEN checks that it is a DataProcessingDefinition/]
			[let dpd : Library::DataProcessingDefinition = ps.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]						[comment OPEN assigns the variable to be a DataProcessingDefinition/]
				[if (dpd.oclIsKindOf(Library::Transformation))]																								[comment OPEN checks if the DataProcessingDefinition is a Transformation/]
					[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]															[comment OPEN assigns the variable to be a Transformation/]
						[for (dc : DataDictionary | ps.inputPort)]																							[comment OPEN read all the input datasets and store them/]
							[if (aWorkflow.environment.oclIsKindOf(Local))]																					[comment OPEN checks if it is a container/]
	[dc.dataDictionaryDefinition.name/]=pd.read_parquet('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[for (st : Storage | ps.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][/if][/for][if (dc.fileName.trim().indexOf('.')>0)][dc.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dc.fileName.trim()/].parquet[/if]')
							[/if]																															[comment CLOSE checks if it is a local file and a container or a local environment/]

						[/for]																																[comment CLOSE read all the input datasets and store them/]
						[for (dc : DataDictionary | ps.outputPort)]																							[comment OPEN read all the output datasets and store them/]
							[if (aWorkflow.environment.oclIsKindOf(Local))]																					[comment OPEN checks if it is a local file and a container/]
	if os.path.exists([if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[for (st : Storage | ps.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][/if][/for][if (dc.fileName.trim().indexOf('.')>0)][dc.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dc.fileName.trim()/].parquet[/if]'):
		[dc.dataDictionaryDefinition.name/]_df=pd.read_parquet('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[for (st : Storage | ps.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][/if][/for][if (dc.fileName.trim().indexOf('.')>0)][dc.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dc.fileName.trim()/].parquet[/if]')
							[/if]																															[comment CLOSE checks if it is a local file and a container or a local environment/]

						[/for]																																[comment CLOSE read all the output datasets and store them/]
						[if (ps.oclIsKindOf(DataProcessing))]
							[let d : DataProcessing = ps.oclAsType(DataProcessing)]
								[for (c : Contract | d.contract)]																							[comment OPEN traverse all the contracts in the DataProcessing/]
									[if (c.contract.type=ContractType::PRECONDITION)]																		[comment OPEN checks if the if the contract is a Precondition/]
	[generateCallContract(c.contract, d)/]
									[/if]																													[comment CLOSE checks if the if the contract is a Precondition/]
								[/for]																														[comment CLOSE traverse all the contracts in the DataProcessing/]
								[for (c : Contract | d.contract)]																							[comment OPEN traverse all the contracts in the DataProcessing/]
									[if (c.contract.type=ContractType::POSTCONDITION)]																		[comment OPEN checks if the if the contract is a Postcondition/]
	[generateCallContract(c.contract, d)/]
									[/if]																													[comment CLOSE checks if the if the contract is a Postcondition/]
								[/for]																														[comment CLOSE traverse all the contracts in the DataProcessing/]
								[for (c : Contract | d.contract)]																							[comment OPEN traverse all the contracts in the DataProcessing/]
									[if (c.contract.type=ContractType::INVARIANT)]																			[comment OPEN checks if the if the contract is a Invariant/]
	[generateCallContract(c.contract, d)/]
									[/if]																													[comment CLOSE checks if the if the contract is a Invariant/]
								[/for]																														[comment CLOSE traverse all the contracts in the DataProcessing/]
							[/let]
						[/if]
						[if (ps.oclIsKindOf(PMMLModel))]
							[let pmml : PMMLModel = ps.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
							[/let]
						[/if]
					[/let]																																	[comment CLOSE assigns the variable to be a Transformation/]
				[elseif (dpd.oclIsKindOf(Library::Job))]																									[comment OPEN checks if the DataProcessingDefinition is a Job/]
					[let libJ : Library::Job = dpd.oclAsType(Library::Job)]																					[comment OPEN assigns the variable to be a Job/]
	#--------------------------------------Input data dictionaries--------------------------------------
						[for (arg : Argument | ps._in)]																										[comment OPEN traverse all the input arguments/]
							[if (arg.oclIsKindOf(DataDictionary))]																							[comment OPEN checks if the argument is a DataDictionary/]
								[let dd_in : DataDictionary = arg.oclAsType(DataDictionary)]																[comment OPEN assigns the variable to be a DataDictionary/]
									[if (aWorkflow.environment.oclIsKindOf(Local))]																			[comment OPEN checks if it is a local file and a container/]
	[dd_in.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[for (st : Storage | ps.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.fileName.trim().indexOf('.')>0)][dd_in.fileName.replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.fileName.trim()/].parquet[/if]'
									[/if]																													[comment CLOSE checks if it is a local file and a container or a local environment/]
								[/let]																														[comment CLOSE assigns the variable to be a DataDictionary/]
							[/if]																															[comment CLOSE checks if the argument is a DataDictionary/]
						[/for]																																[comment CLOSE traverse all the input arguments/]
	#--------------------------------------Output data dictionaries--------------------------------------
						[for (arg : Argument | ps.out)]																										[comment OPEN traverse all the output arguments/]
							[if (arg.oclIsKindOf(DataDictionary))]																							[comment OPEN checks if the argument is a DataDictionary/]
								[let dd_out : DataDictionary = arg.oclAsType(DataDictionary)]																[comment OPEN assigns the variable to be a DataDictionary/]
									[if (aWorkflow.environment.oclIsKindOf(Local))]																			[comment OPEN checks if it is a local file and a container/]
	[dd_out.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[for (st : Storage | ps.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_out.fileName.trim().indexOf('.')>0)][dd_out.fileName.replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_out.fileName.trim()/].parquet[/if]'
									[/if]																													[comment CLOSE checks if it is a local file and a container or a local environment/]
								[/let]																														[comment CLOSE assigns the variable to be a DataDictionary/]
							[/if]																															[comment CLOSE checks if the argument is a DataDictionary/]
						[/for]																																[comment CLOSE traverse all the input arguments/]

						[if (ps.oclIsKindOf(DataProcessing))]
							[let d : DataProcessing = ps.oclAsType(DataProcessing)]
								[for (pstep : ProcessingStep | libJ.workflow.dataprocessing)]																[comment OPEN traverse all the dataProcessing in the workflow/]
									[if (pstep.incoming = null and pstep.outgoing=null)]																	[comment OPEN checks if the dataProcessing is isolated/]
										[if (pstep.oclIsKindOf(DataProcessing))]
											[let dataP : DataProcessing = pstep.oclAsType(DataProcessing)]	
												[for (c : Contract | dataP.contract)]																		[comment OPEN traverse all the contracts in the DataProcessing/]
													[if (c.contract.type=ContractType::PRECONDITION)]														[comment OPEN checks if the if the contract is a Precondition/]
	[generateCallContract(c.contract, d)/]
													[/if]																									[comment CLOSE checks if the if the contract is a Precondition/]
												[/for]																										[comment CLOSE traverse all the contracts in the DataProcessing/]
												[for (c : Contract | dataP.contract)]																		[comment OPEN traverse all the contracts in the DataProcessing/]
													[if (c.contract.type=ContractType::POSTCONDITION)]														[comment OPEN checks if the if the contract is a Postcondition/]
	[generateCallContract(c.contract, d)/]
													[/if]																									[comment CLOSE checks if the if the contract is a Postcondition/]
												[/for]																										[comment CLOSE traverse all the contracts in the DataProcessing/]
												[for (c : Contract | dataP.contract)]																		[comment OPEN traverse all the contracts in the DataProcessing/]
													[if (c.contract.type=ContractType::INVARIANT)]															[comment OPEN checks if the if the contract is a Invariant/]
	[generateCallContract(c.contract, d)/]
													[/if]																									[comment CLOSE checks if the if the contract is a Invariant/]
												[/for]																										[comment CLOSE traverse all the contracts in the DataProcessing/]
											[/let]
										[/if]
										[if (pstep.oclIsKindOf(PMMLModel))]
											[let pmml : PMMLModel = pstep.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
											[/let]
										[/if]
									[/if]																													[comment CLOSE checks if the dataProcessing is isolated/]
								[/for]																														[comment CLOSE traverse all the dataProcessing in the workflow/]
								[for (pstep : ProcessingStep | libJ.workflow.dataprocessing)]																[comment OPEN traverse all the dataProcessing in the workflow/]
									[if (pstep.incoming = null and pstep.outgoing<>null)]																	[comment OPEN checks if it is the first dataProcessing/]
										[if (pstep.oclIsKindOf(DataProcessing))]
											[let dataP : DataProcessing = pstep.oclAsType(DataProcessing)]
[callContractsModifiedRecursiveTemplate(dataP, d)/]
											[/let]
										[/if]																												[comment CLOSE checks if it is the first dataProcessing/]
										[if (pstep.oclIsKindOf(PMMLModel))]
											[let pmml : PMMLModel = pstep.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
											[/let]
										[/if]
									[/if]
								[/for]																														[comment CLOSE traverse all the dataProcessing in the workflow/]
							[/let]
						[/if]																																[comment CLOSE checks if it is the first dataProcessing/]
						[if (ps.oclIsKindOf(PMMLModel))]
							[let pmml : PMMLModel = ps.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
							[/let]
						[/if]
					[/let]																																	[comment CLOSE assigns the variable to be a Transformation/]
				[/if]																																		[comment CLOSE checks if the DataProcessingDefinition is a Transformation or a Job/]
			[/let]																																			[comment CLOSE assigns the variable to be a DataProcessingDefinition/]
		[/if]																																				[comment CLOSE checks that it is a DataProcessingDefinition/]
	[/if]																																					[comment CLOSE checks if the DataProcessing is isolated (no links)/]
[/for]																																						[comment CLOSE traverse all the DataProcessings/]
[for (ps : ProcessingStep | aWorkflow.dataprocessing)]																										[comment OPEN traverse all the DataProcessings/]
	[if (ps.incoming = null and ps.outgoing<>null)]																											[comment OPEN checks if the DataProcessing is the first (no input link, an output link)/]
		[if (ps.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]																	[comment OPEN assigns the variable to be a DataProcessingDefinition/]
			[let dpd : Library::DataProcessingDefinition = ps.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]						[comment OPEN assigns the variable to be a DataProcessingDefinition/]
				[if (dpd.oclIsKindOf(Library::Transformation))]																								[comment OPEN checks if the DataProcessingDefinition is a Transformation/]
					[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]															[comment OPEN assigns the variable to be a Transformation/]
						[if (ps.oclIsKindOf(DataProcessing))]
							[let d : DataProcessing = ps.oclAsType(DataProcessing)]
[callContractsRecursiveTemplate(d)/]
							[/let]
						[/if]
						[if (ps.oclIsKindOf(PMMLModel))]
							[let pmml : PMMLModel = ps.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
							[/let]
						[/if]
					[/let]																																	[comment CLOSE assigns the variable to be a Transformation/]
				[elseif (dpd.oclIsKindOf(Library::Job))]																									[comment OPEN assigns the variable to be a Job/]
					[let libJ : Library::Job = dpd.oclAsType(Library::Job)]																					[comment OPEN assigns the variable to be a Job/]
	#--------------------------------------Input data dictionaries--------------------------------------
						[for (arg : Argument | ps._in)]																										[comment OPEN traverse all the input arguments/]
							[if (arg.oclIsKindOf(DataDictionary))]																							[comment OPEN checks if the argument is a DataDictionary/]
								[let dd_in : DataDictionary = arg.oclAsType(DataDictionary)]																[comment OPEN assigns the variable to be a DataDictionary/]
									[if (aWorkflow.environment.oclIsKindOf(Local))]																			[comment OPEN checks if it is a local file and a container/]
	[dd_in.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[for (st : Storage | ps.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.fileName.trim().indexOf('.')>0)][dd_in.fileName.replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.fileName.trim()/].parquet[/if]'
									[/if]																													[comment CLOSE checks if it is a local file and a container or a local environment/]
								[/let]																														[comment CLOSE assigns the variable to be a DataDictionary/]
							[/if]																															[comment CLOSE checks if the argument is a DataDictionary/]
						[/for]																																[comment CLOSE traverse all the input arguments/]
	#--------------------------------------Output data dictionaries--------------------------------------
						[for (arg : Argument | ps.out)]																										[comment OPEN traverse all the output arguments/]
							[if (arg.oclIsKindOf(DataDictionary))]																							[comment OPEN checks if the argument is a DataDictionary/]
								[let dd_out : DataDictionary = arg.oclAsType(DataDictionary)]																[comment OPEN assigns the variable to be a DataDictionary/]
									[if (aWorkflow.environment.oclIsKindOf(Local))]																			[comment OPEN checks if it is a local file and a container/]
	[dd_out.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[for (st : Storage | ps.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_out.fileName.trim().indexOf('.')>0)][dd_out.fileName.replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_out.fileName.trim()/].parquet[/if]'
									[/if]																													[comment CLOSE checks if it is a local file and a container or a local environment/]
								[/let]																														[comment CLOSE assigns the variable to be a DataDictionary/]
							[/if]																															[comment CLOSE checks if the argument is a DataDictionary/]
						[/for]																																[comment CLOSE traverse all the output arguments/]

						[if (ps.oclIsKindOf(DataProcessing))]
							[let d : DataProcessing = ps.oclAsType(DataProcessing)]
								[for (pstep : ProcessingStep | libJ.workflow.dataprocessing)]																[comment OPEN traverse all the DataProcessings/]
									[if (pstep.incoming = null and pstep.outgoing=null)]																	[comment OPEN checks if the dataProcessing is isolated/]
										[if (pstep.oclIsKindOf(DataProcessing))]
											[let dataP : DataProcessing = pstep.oclAsType(DataProcessing)]
												[for (c : Contract | dataP.contract)]																		[comment OPEN traverse all the contracts in the DataProcessing/]
													[if (c.contract.type=ContractType::PRECONDITION)]														[comment OPEN checks if the if the contract is a Precondition/]
	[generateCallContract(c.contract, d)/]
													[/if]																									[comment CLOSE checks if the if the contract is a Precondition/]
												[/for]																										[comment CLOSE traverse all the contracts in the DataProcessing/]
												[for (c : Contract | dataP.contract)]																		[comment OPEN traverse all the contracts in the DataProcessing/]
													[if (c.contract.type=ContractType::POSTCONDITION)]														[comment OPEN checks if the if the contract is a Postcondition/]
	[generateCallContract(c.contract, d)/]
													[/if]																									[comment CLOSE checks if the if the contract is a Postcondition/]
												[/for]																										[comment CLOSE traverse all the contracts in the DataProcessing/]
												[for (c : Contract | dataP.contract)]																		[comment OPEN traverse all the contracts in the DataProcessing/]
													[if (c.contract.type=ContractType::INVARIANT)]															[comment OPEN checks if the if the contract is a Invariant/]
	[generateCallContract(c.contract, d)/]
													[/if]																									[comment CLOSE checks if the if the contract is a Invariant/]
												[/for]																										[comment CLOSE traverse all the contracts in the DataProcessing/]
											[/let]
										[/if]
										[if (pstep.oclIsKindOf(PMMLModel))]
											[let pmml : PMMLModel = pstep.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
											[/let]
										[/if]
									[/if]																													[comment CLOSE checks if the dataProcessing is isolated/]
								[/for]																														[comment CLOSE traverse all the DataProcessings/]
								[for (pstep : ProcessingStep | libJ.workflow.dataprocessing)]																[comment OPEN traverse all the dataProcessing in the workflow/]
									[if (pstep.incoming = null and pstep.outgoing<>null)]																	[comment OPEN checks if it is the first dataProcessing/]
										[if (pstep.oclIsKindOf(DataProcessing))]
											[let dataP : DataProcessing = pstep.oclAsType(DataProcessing)]
[callContractsModifiedRecursiveTemplate(dataP, d)/]
											[/let]
										[/if]																												[comment CLOSE checks if it is the first dataProcessing/]
										[if (pstep.oclIsKindOf(PMMLModel))]
											[let pmml : PMMLModel = pstep.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
											[/let]
										[/if]
									[/if]
								[/for]																														[comment CLOSE traverse all the DataProcessings/]
							[/let]
						[/if]
						[if (ps.oclIsKindOf(PMMLModel))]
							[let pmml : PMMLModel = ps.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
							[/let]
						[/if]
					[/let]																																	[comment CLOSE assigns the variable to be a Transformation/]
				[/if]																																		[comment CLOSE checks if the DataProcessingDefinition is a Transformation or a Job/]
			[/let]																																			[comment CLOSE assigns the variable to be a DataProcessingDefinition/]
		[/if]																																				[comment CLOSE assigns the variable to be a DataProcessingDefinition/]
	[/if]																																					[comment CLOSE checks if the DataProcessing is the first (no input link, an output link)/]
[/for]																																						[comment CLOSE traverse all the DataProcessings/]
set_logger("contracts")
generateWorkflow()
[/file]
[/template]


[template public callContractsModifiedRecursiveTemplate(dw : DataProcessing, d_or : DataProcessing)]
	#-----------------New DataProcessing-----------------
[if (dw.incoming <> null)]															[comment OPEN checks if there is a previous DataProcessing/]
	[if (dw.incoming.source.dataProcessingDefinition.name<>'split')]				[comment OPEN checks that the operation is not split/]
		[for (dc : DataDictionary | dw.inputPort)]									[comment OPEN read all the input datasets and store them/]
			[if (d_or.eContainer(Workflow).environment.oclIsKindOf(Local))]			[comment OPEN checks if it is a local file and a container/]
	if os.path.exists([dc.name.replaceAll('[(),\\s]+', '_')/]):
		[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet([dc.name.replaceAll('[(),\\s]+', '_')/])
						
			[/if]
		[/for]

		[for (dc : DataDictionary | dw.outputPort)]									[comment OPEN read all the output datasets and store them/]
			[if (d_or.eContainer(Workflow).environment.oclIsKindOf(Local))]			[comment OPEN checks if it is a local file and a container/]
	if os.path.exists([dc.name.replaceAll('[(),\\s]+', '_')/]):
		[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet([dc.name.replaceAll('[(),\\s]+', '_')/])
			[/if]
			
		[/for]
	[else]																			[comment OPEN checks if the operation is split/]
		[for (dc : DataDictionary | dw.inputPort)]									[comment OPEN read all the input datasets and store them/]
			[for (dc_out : DataDictionary | dw.incoming.source.outputPort)]			[comment OPEN read the previous DataProcessing output datasets/]
				[if (dc.fileName.trim() = dc_out.fileName)]								[comment OPEN checks that both datasets are the same/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet([dc.name.replaceAll('[(),\\s]+', '_')/])

				[/if]																[comment CLOSE checks that both datasets are the same/]
			[/for]																	[comment CLOSE read the previous DataProcessing output datasets/]
		[/for]																		[comment CLOSE read all the input datasets and store them/]
	[/if]																			[comment CLOSE checks if the operation is split or not/]
[else]																				[comment OPEN checks that there is not a previous DataProcessing/]
	[for (dc : DataDictionary | dw.inputPort)]										[comment OPEN read all the input datasets and store them/]
		[if (d_or.eContainer(Workflow).environment.oclIsKindOf(Local))]				[comment OPEN checks if it is a local file and a container/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet([dc.name.replaceAll('[(),\\s]+', '_')/])
		[/if]

	[/for]																			[comment CLOSE read all the input datasets and store them/]
	[for (dc : DataDictionary | dw.outputPort)]										[comment OPEN read all the output datasets and store them/]
		[if (d_or.eContainer(Workflow).environment.oclIsKindOf(Local))]				[comment OPEN checks if it is a local file and a container/]
	if os.path.exists([dc.name.replaceAll('[(),\\s]+', '_')/]):
		[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet([dc.name.replaceAll('[(),\\s]+', '_')/])
		[/if]
			
	[/for]
[/if]																				[comment CLOSE checks if there is a previous DataProcessingor not/]
[for (c : Contract | dw.contract)]													[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::PRECONDITION)]								[comment OPEN checks if the if the contract is a Precondition/]
	[generateCallContract(c.contract, d_or)/]
	[/if]																			[comment CLOSE checks if the if the contract is a Precondition/]
[/for]																				[comment CLOSE traverse all the contracts in the DataProcessing/]
[for (c : Contract | dw.contract)]													[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::POSTCONDITION)]								[comment OPEN checks if the if the contract is a Postcondition/]
	[generateCallContract(c.contract, d_or)/]
	[/if]																			[comment CLOSE checks if the if the contract is a Postcondition/]
[/for]																				[comment CLOSE traverse all the contracts in the DataProcessing/]
[for (c : Contract | dw.contract)]													[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::INVARIANT)]									[comment OPEN checks if the if the contract is a Invariant/]
	[generateCallContract(c.contract, d_or)/]
	[/if]																			[comment CLOSE checks if the if the contract is a Invariant/]
[/for]																				[comment CLOSE traverse all the contracts in the DataProcessing/]
[if (dw.outgoing<>null)]															[comment OPEN checks if the DataProcessing has an outgoing link (it's not the last)/]
	[let nextDp : DataProcessing = dw.outgoing.target.oclAsType(DataProcessing)] 	[comment OPEN assigns the next DataProcessing to a variable/]
[callContractsModifiedRecursiveTemplate(nextDp, d_or)/]			[comment Recursive call to this template to generate all the DataProcessings linked to the first/]
	[/let]																			[comment OPEN assigns the next DataProcessing to a variable/]
[/if]																				[comment CLOSE checks if the DataProcessing has an outgoing link (it's not the last)/]
[/template]


[template public callContractsRecursiveTemplate(d : DataProcessing)]
	#-----------------New DataProcessing-----------------
[if (d.incoming <> null)]																								[comment OPEN checks if there is a previous DataProcessing/]
	[if (d.incoming.source.dataProcessingDefinition.name<>'split')]														[comment OPEN checks that the operation is not split/]
		[if (d.eContainer(Workflow).environment.oclIsKindOf(Local))]													[comment OPEN checks if it is a local file and a container/]
	[d.inputPort->first().name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | d.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=d.inputPort->first())][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=d.inputPort->first())][st.storageName/]/[/if][/for][/let][/if][/for][if (d.inputPort->first().fileName.trim().indexOf('.')>0)][d.inputPort->first().fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][d.inputPort->first().fileName.trim()/].parquet[/if]')
		[/if]																											[comment CLOSE checks if it is a local file and a container or a local environment/]

	[else]																												[comment OPEN checks if the operation is split/]
		[for (dc : DataDictionary | d.inputPort)]																		[comment OPEN read all the input datasets and store them/]
			[for (dc_out : DataDictionary | d.incoming.source.outputPort)]												[comment OPEN read the previous DataProcessing output datasets/]
				[if (dc.fileName.trim() = dc_out.fileName)]																	[comment OPEN checks that both datasets are the same/]
					[if (d.eContainer(Workflow).environment.oclIsKindOf(Local))]										[comment OPEN checks if it is a local file and a container/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | d.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dc_out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dc_out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dc_out.fileName.trim().indexOf('.')>0)][dc_out.fileName.replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dc_out.fileName/].parquet[/if]')
					[/if]																								[comment CLOSE checks if it is a local file and a container or a local environment/]

				[/if]																									[comment CLOSE checks that both datasets are the same/]
			[/for]																										[comment CLOSE read the previous DataProcessing output datasets/]
		[/for]																											[comment CLOSE read all the input datasets and store them/]
	[/if]																												[comment CLOSE checks that the operation is split or not/]
[else]																													[comment OPEN checks that there is not a previous DataProcessing/]
	[for (dc : DataDictionary | d.inputPort)]																			[comment OPEN read all the input datasets and store them/]
		[if (d.eContainer(Workflow).environment.oclIsKindOf(Local))]													[comment OPEN checks if it is a local file and a container/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | d.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][/if][/for][if (dc.fileName.trim().indexOf('.')>0)][dc.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dc.fileName.trim()/].parquet[/if]')
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | d.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][/if][/for][if (dc.fileName.trim().indexOf('.')>0)][dc.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dc.fileName.trim()/].parquet[/if]')
		[/if]																											[comment CLOSE checks if it is a local file and a container or a local environment/]
	[/for]																												[comment CLOSE read all the input datasets and store them/]
[/if]																													[comment CLOSE checks if there is a previous DataProcessingor not/]
[for (dc : DataDictionary | d.outputPort)]																				[comment OPEN read all the output datasets and store them/]
	[if (d.eContainer(Workflow).environment.oclIsKindOf(Local))]														[comment OPEN checks if it is a local file and a container/]
	if os.path.exists('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | d.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][/if][/for][if (dc.fileName.trim().indexOf('.')>0)][dc.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dc.fileName.trim()/].parquet[/if]'):
		[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | d.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][/if][/for][if (dc.fileName.trim().indexOf('.')>0)][dc.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dc.fileName.trim()/].parquet[/if]')
	[/if]																												[comment CLOSE checks if it is a local file and a container or a local environment/]

[/for]																													[comment CLOSE read all the output datasets and store them/]
[for (c : Contract | d.contract)]																						[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::PRECONDITION)]																	[comment OPEN checks if the if the contract is a Precondition/]
	[generateCallContract(c.contract, d)/]
	[/if]																												[comment CLOSE checks if the if the contract is a Precondition/]
[/for]																													[comment CLOSE traverse all the contracts in the DataProcessing/]
[for (c : Contract | d.contract)]																						[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::POSTCONDITION)]																	[comment OPEN checks if the if the contract is a Postcondition/]
	[generateCallContract(c.contract, d)/]
	[/if]																												[comment CLOSE checks if the if the contract is a Postcondition/]
[/for]																													[comment CLOSE traverse all the contracts in the DataProcessing/]
[for (c : Contract | d.contract)]																						[comment OPEN traverse all the contracts in the DataProcessing/]
	[if (c.contract.type=ContractType::INVARIANT)]																		[comment OPEN checks if the if the contract is a Invariant/]
	[generateCallContract(c.contract, d)/]
	[/if]																												[comment CLOSE checks if the if the contract is a Invariant/]
[/for]																													[comment CLOSE traverse all the contracts in the DataProcessing/]
[if (d.outgoing<>null)]																									[comment OPEN checks if the DataProcessing has an outgoing link (it's not the last)/]
	[let nextDp : DataProcessing = d.outgoing.target.oclAsType(DataProcessing)]											[comment OPEN assigns the next DataProcessing to a variable/]
[callContractsRecursiveTemplate(nextDp)/]											[comment Recursive call to this template to generate all the DataProcessings linked to the first/]
	[/let]																												[comment OPEN assigns the next DataProcessing to a variable/]
[/if]																													[comment CLOSE checks if the DataProcessing has an outgoing link (it's not the last)/]
[/template]


[template public generateTransformationsDataProcessing(aWorkflow : Workflow)]
[file ('transformations_Job_'+aWorkflow.name.replaceAll('[(),\\s]+', '_')+'.py', false, 'UTF-8')]
import pandas as pd
import numpy as np
import functions.data_transformations as data_transformations
from helpers.enumerations import Belong, Operator, Operation, SpecialType, DataType, DerivedType, Closure, FilterType
from helpers.logger import set_logger
import pyarrow

[comment generate the imports and the class/]
def generateWorkflow():
[for (ps : ProcessingStep | aWorkflow.dataprocessing)]																					[comment OPEN traverse all the DataProcessings/]
	[if (ps.incoming = null and ps.outgoing=null)]																						[comment OPEN checks if the DataProcessing is isolated (no links)/]
	#-----------------New DataProcessing-----------------
		[if (ps.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]												[comment OPEN checks if the dataProcessingDefinition is a dataProcessingDefinition/]
			[let dpd : Library::DataProcessingDefinition = ps.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]		[comment OPEN assigns the variable to be a dataProcessingDefinition/]
				[if (dpd.oclIsKindOf(Library::Transformation))]																			[comment OPEN checks if the dataProcessingDefinition is a Transformation/]
					[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]										[comment OPEN assigns the variable to be a Tranformation/]
						[for (dc : DataDictionary | ps.inputPort)]																		[comment OPEN read all the input datasets and store them/]
							[if (aWorkflow.environment.oclIsKindOf(Local))]																[comment OPEN checks if it is a local file and a container/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[for (st : Storage | ps.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][/if][/for][if (dc.fileName.trim().indexOf('.')>0)][dc.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dc.fileName.trim()/].parquet[/if]')
							[/if]																										[comment CLOSE checks if it is a local file and a container or a local environment/]

						[/for]																											[comment CLOSE read all the input datasets and store them/]
						[if (ps.oclIsKindOf(DataProcessing))]
							[let d : DataProcessing = ps.oclAsType(DataProcessing)]
[generateCallTransformation(d, d)/]
							[/let]
						[/if]
						[if (ps.oclIsKindOf(PMMLModel))]
							[let pmml : PMMLModel = ps.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
							[/let]
						[/if]
					[/let]																												[comment CLOSE assigns the variable to be a Tranformation/]
				[elseif (dpd.oclIsKindOf(Library::Job))]																				[comment OPEN checks if the dataProcessingDefinition is a Job/]
					[let libJ : Library::Job = dpd.oclAsType(Library::Job)]																[comment OPEN assigns the variable to be a Job/]
	#--------------------------------------Input data dictionaries--------------------------------------
						[for (arg : Argument | ps._in)]																					[comment OPEN traverse all the input arguments/]
							[if (arg.oclIsKindOf(DataDictionary))]																		[comment OPEN checks if it is a DataDictionary/]
								[let dd_in : DataDictionary = arg.oclAsType(DataDictionary)]											[comment OPEN assigns the variable to be a DataDictionary/]
									[if (aWorkflow.environment.oclIsKindOf(Local))]														[comment OPEN checks if it is a local file and a container/]
	[dd_in.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[for (st : Storage | ps.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.fileName.trim().indexOf('.')>0)][dd_in.fileName.replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.fileName.trim()/].parquet[/if]'
									[/if]																								[comment CLOSE checks if it is a local file and a container or a local environment/]
								[/let]																									[comment CLOSE assigns the variable to be a DataDictionary/]
							[/if]																										[comment CLOSE checks if it is a DataDictionary/]
						[/for]																											[comment CLOSE traverse all the input arguments/]
	#--------------------------------------Output data dictionaries--------------------------------------
						[for (arg : Argument | ps.out)]																					[comment OPEN traverse all the output arguments/]
							[if (arg.oclIsKindOf(DataDictionary))]																		[comment OPEN checks if it is a DataDictionary/]
								[let dd_out : DataDictionary = arg.oclAsType(DataDictionary)]											[comment OPEN assigns the variable to be a DataDictionary/]
									[if (aWorkflow.environment.oclIsKindOf(Local))]														[comment OPEN checks if it is a local file and a container/]
	[dd_out.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[for (st : Storage | ps.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_out.fileName.trim().indexOf('.')>0)][dd_out.fileName.replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_out.fileName.trim()/].parquet[/if]'
									[/if]																								[comment CLOSE checks if it is a local file and a container or a local environment/]
								[/let]																									[comment CLOSE assigns the variable to be a DataDictionary/]
							[/if]																										[comment CLOSE checks if it is a DataDictionary/]
						[/for]																											[comment CLOSE traverse all the output arguments/]
						[if (ps.oclIsKindOf(DataProcessing))]
							[let d : DataProcessing = ps.oclAsType(DataProcessing)]
								[for (pstep : ProcessingStep | libJ.workflow.dataprocessing)]														[comment OPEN traverse all the dataProcessing/]
									[if (pstep.oclIsKindOf(DataProcessing))]
										[let dp : DataProcessing = pstep.oclAsType(DataProcessing)]
											[if (pstep.incoming = null and pstep.outgoing=null)]																[comment OPEN checks if it is isolated/]
[generateCallTransformation(dp, d)/]
											[/if]																										[comment CLOSE checks if it is isolated/]
										[/let]
									[/if]
									[if (pstep.oclIsKindOf(PMMLModel))]
										[let pmml : PMMLModel = pstep.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
										[/let]
									[/if]
								[/for]																											[comment CLOSE traverse all the dataProcessing/]
								[for (pstep : ProcessingStep | libJ.workflow.dataprocessing)]														[comment OPEN traverse all the dataProcessing/]
									[if (pstep.oclIsKindOf(DataProcessing))]
										[let dp : DataProcessing = pstep.oclAsType(DataProcessing)]
											[if (pstep.incoming = null and pstep.outgoing<>null)]																[comment OPEN checks if it is the first dataProcessing/]
[callTransformationsModifiedRecursiveTemplate(dp, d)/]
											[/if]																										[comment CLOSE checks if it is not isolated/]
										[/let]
									[/if]
									[if (pstep.oclIsKindOf(PMMLModel))]
										[let pmml : PMMLModel = pstep.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
										[/let]
									[/if]																										[comment CLOSE checks if it is the first dataProcessing/]
								[/for]																											[comment CLOSE traverse all the dataProcessing/]
							[/let]
						[/if]
						[if (ps.oclIsKindOf(PMMLModel))]
							[let pmml : PMMLModel = ps.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
							[/let]
						[/if]
					[/let]																												[comment CLOSE assigns the variable to be a Job/]
				[/if]																													[comment CLOSE checks if the dataProcessingDefinition is a Transformation or a Job/]
			[/let]																														[comment CLOSE assigns the variable to be a dataProcessingDefinition/]
		[/if]																															[comment CLOSE checks if the dataProcessingDefinition is a dataProcessingDefinition/]
	[/if]																																[comment CLOSE checks if the DataProcessing is isolated (no links)/]
[/for]																																	[comment CLOSE traverse all the DataProcessings/]
[for (ps : ProcessingStep | aWorkflow.dataprocessing)]																					[comment OPEN traverse all the DataProcessings/]
	[if (ps.incoming = null and ps.outgoing<>null)]																						[comment OPEN checks if the DataProcessing is the first (no input link, an output link)/]
		[if (ps.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]												[comment OPEN checks if the dataProcessingDefinition is a dataProcessingDefinition/]
			[let dpd : Library::DataProcessingDefinition = ps.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]		[comment OPEN assigns the variable to be a dataProcessingDefinition/]
				[if (dpd.oclIsKindOf(Library::Transformation))]																			[comment OPEN checks if the dataProcessingDefinition is a Transformation/]
					[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]										[comment OPEN assigns the variable to be a Tranformation/]
						[if (ps.oclIsKindOf(DataProcessing))]
							[let d : DataProcessing = ps.oclAsType(DataProcessing)]
[callTransformationsRecursiveTemplate(d)/]
							[/let]
						[/if]
						[if (ps.oclIsKindOf(PMMLModel))]
							[let pmml : PMMLModel = ps.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
							[/let]
						[/if]
					[/let]																												[comment CLOSE assigns the variable to be a Tranformation/]
				[elseif (dpd.oclIsKindOf(Library::Job))]																				[comment OPEN checks if the dataProcessingDefinition is a Job/]
					[let libJ : Library::Job = dpd.oclAsType(Library::Job)]																[comment OPEN assigns the variable to be a Job/]
	#--------------------------------------Input data dictionaries--------------------------------------
						[for (arg : Argument | ps._in)]																					[comment OPEN traverse all the input arguments/]
							[if (arg.oclIsKindOf(DataDictionary))]																		[comment OPEN checks if it is a DataDictionary/]
								[let dd_in : DataDictionary = arg.oclAsType(DataDictionary)]											[comment OPEN assigns the variable to be a DataDictionary/]
									[if (aWorkflow.environment.oclIsKindOf(Local))]														[comment OPEN checks if it is a local file and a container/]
	[dd_in.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[for (st : Storage | ps.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.fileName.trim().indexOf('.')>0)][dd_in.fileName.replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.fileName.trim()/].parquet[/if]'
									[/if]																								[comment CLOSE checks if it is a local file and a container or a local environment/]
								[/let]																									[comment CLOSE assigns the variable to be a DataDictionary/]
							[/if]																										[comment CLOSE checks if it is a DataDictionary/]
						[/for]																											[comment CLOSE traverse all the input arguments/]
	#--------------------------------------Output data dictionaries--------------------------------------
						[for (arg : Argument | ps.out)]																					[comment OPEN traverse all the output arguments/]
							[if (arg.oclIsKindOf(DataDictionary))]																		[comment OPEN checks if it is a DataDictionary/]
								[let dd_out : DataDictionary = arg.oclAsType(DataDictionary)]											[comment OPEN assigns the variable to be a DataDictionary/]
									[if (aWorkflow.environment.oclIsKindOf(Local))]														[comment OPEN checks if it is a local file and a container/]
	[dd_out.dataDictionaryDefinition.boundTo.name.replaceAll('[(),\\s]+', '_')/]='[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[for (st : Storage | ps.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_out.fileName.trim().indexOf('.')>0)][dd_out.fileName.replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_out.fileName.trim()/].parquet[/if]'
									[/if]																								[comment CLOSE checks if it is a local file and a container or a local environment/]
								[/let]																									[comment CLOSE assigns the variable to be a DataDictionary/]
							[/if]																										[comment CLOSE checks if it is a DataDictionary/]
						[/for]																											[comment CLOSE traverse all the output arguments/]
						[if (ps.oclIsKindOf(DataProcessing))]
							[let d : DataProcessing = ps.oclAsType(DataProcessing)]
								[for (pstep : ProcessingStep | libJ.workflow.dataprocessing)]														[comment OPEN traverse all the dataProcessing/]
									[if (pstep.incoming = null and pstep.outgoing=null)]																[comment OPEN checks if it is isolated/]
										[if (pstep.oclIsKindOf(DataProcessing))]
											[let dp : DataProcessing = pstep.oclAsType(DataProcessing)]
[generateCallTransformation(dp, d)/]
											[/let]
										[/if]
										[if (pstep.oclIsKindOf(PMMLModel))]
											[let pmml : PMMLModel = pstep.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
											[/let]
										[/if]
									[/if]																										[comment CLOSE checks if it is isolated/]
								[/for]																											[comment CLOSE traverse all the dataProcessing/]
								[for (pstep : ProcessingStep | libJ.workflow.dataprocessing)]														[comment OPEN traverse all the dataProcessing/]
									[if (pstep.incoming = null and pstep.outgoing<>null)]																[comment OPEN checks if it is the first dataProcessing/]
										[if (pstep.oclIsKindOf(DataProcessing))]
											[let dp : DataProcessing = pstep.oclAsType(DataProcessing)]
[callTransformationsModifiedRecursiveTemplate(dp, d)/]
											[/let]
										[/if]
										[if (pstep.oclIsKindOf(PMMLModel))]
											[let pmml : PMMLModel = pstep.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
											[/let]
										[/if]
									[/if]																										[comment CLOSE checks if it is the first dataProcessing/]
								[/for]																											[comment CLOSE traverse all the dataProcessing/]
							[/let]
						[/if]
						[if (ps.oclIsKindOf(PMMLModel))]
							[let pmml : PMMLModel = ps.oclAsType(PMMLModel)]
	[generatePMML(pmml)/]
							[/let]
						[/if]
					[/let]																												[comment CLOSE assigns the variable to be a Tranformation/]
				[/if]																													[comment CLOSE checks if the dataProcessingDefinition is a Transformation or a Job/]
			[/let]																														[comment CLOSE assigns the variable to be a dataProcessingDefinition/]
		[/if]																															[comment CLOSE checks if the dataProcessingDefinition is a dataProcessingDefinition/]
	[/if]																																[comment CLOSE checks if the DataProcessing is the first (no input link, an output link)/]
[/for]																																	[comment CLOSE traverse all the DataProcessings/]
set_logger("transformations")
generateWorkflow()
[/file]
[/template]


[template public callTransformationsModifiedRecursiveTemplate(dw : DataProcessing, d_or : DataProcessing)]
	#-----------------New DataProcessing-----------------
[if (dw.incoming <> null)]																[comment OPEN checks if there is an incoming link/]
	[if (dw.incoming.source.dataProcessingDefinition.name<>'split')]					[comment OPEN checks if the operation is split/]
	[dw.inputPort->first().name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet([dw.inputPort->first().name.replaceAll('[(),\\s]+', '_')/])

	[else]																				[comment OPEN checks that the operation is not split/]
		[for (dc : DataDictionary | dw.inputPort)]										[comment OPEN read all the input datasets and store them/]
			[for (dc_out : DataDictionary | dw.incoming.source.outputPort)]				[comment OPEN read the previous DataProcessing output datasets/]
				[if (dc.fileName.trim() = dc_out.fileName)]									[comment OPEN checks that both datasets are the same/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet([dc.name.replaceAll('[(),\\s]+', '_')/])

				[/if]																	[comment CLOSE checks that both datasets are the same/]
			[/for]																		[comment CLOSE read the previous DataProcessing output datasets/]
		[/for]																			[comment CLOSE read all the input datasets and store them/]
	[/if]																				[comment CLOSE checks if the operation is split or not/]
[else]																					[comment OPEN checks if there is not an incoming link/]
	[for (dc : DataDictionary | dw.inputPort)]											[comment OPEN read all the input datasets and store them/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet([dc.name.replaceAll('[(),\\s]+', '_')/])
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet([dc.name.replaceAll('[(),\\s]+', '_')/])
	[/for]																				[comment CLOSE read all the input datasets and store them/]
[/if]																					[comment CLOSE checks if there is an incoming link or not/]
	[generateCallTransformation(dw, d_or)/]
[if (dw.outgoing<>null)]																[comment OPEN checks if the DataProcessing has an outgoing link (it's not the last)/]
	[let nextDp : DataProcessing = dw.outgoing.target.oclAsType(DataProcessing)] 		[comment OPEN assigns the next DataProcessing to a variable/]
[callTransformationsModifiedRecursiveTemplate(nextDp, d_or)/]
	[/let]																				[comment OPEN assigns the next DataProcessing to a variable/]
[/if]																					[comment CLOSE checks if the DataProcessing has an outgoing link (it's not the last)/]
[/template]


[template public callTransformationsRecursiveTemplate(d : DataProcessing)]
	#-----------------New DataProcessing-----------------
[if (d.incoming <> null)]																																		[comment OPEN checks if there is an incoming link/]
	[if (d.incoming.source.dataProcessingDefinition.name<>'split')]																								[comment OPEN checks if the operation is split/]
		[if (d.eContainer(Workflow).environment.oclIsKindOf(Local))]																							[comment OPEN checks if it is a local file and a container/]
	[d.inputPort->first().name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | d.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=d.inputPort->first())][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=d.inputPort->first())][st.storageName/]/[/if][/for][/let][/if][/for][if (d.inputPort->first().fileName.trim().indexOf('.')>0)][d.inputPort->first().fileName.replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][d.inputPort->first().fileName/].parquet[/if]')
		[/if]																																					[comment CLOSE checks if it is a local file and a local container/]

	[else]																																						[comment OPEN checks that the operation is not split/]
		[for (dc : DataDictionary | d.inputPort)]																												[comment OPEN read all the input datasets and store them/]
			[for (dc_out : DataDictionary | d.incoming.source.outputPort)]																						[comment OPEN read the previous DataProcessing output datasets/]
				[if (dc.fileName.trim() = dc_out.fileName)]																											[comment OPEN checks that both datasets are the same/]
					[if (d.eContainer(Workflow).environment.oclIsKindOf(Local))]																				[comment OPEN checks if it is a local file and a container/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | d.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary.fileName=dc_out.fileName)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary.fileName=dc_out.fileName)][st.storageName/]/[/if][/for][/let][/if][/for][if (dc_out.fileName.trim().indexOf('.')>0)][dc_out.fileName.replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dc_out.fileName/].parquet[/if]')
					[/if]																																		[comment CLOSE checks if it is a local file and a container or a local environment/]
				[/if]																																			[comment CLOSE checks that both datasets are the same/]
			[/for]																																				[comment CLOSE read the previous DataProcessing output datasets/]
		[/for]																																					[comment CLOSE read all the input datasets and store them/]
	[/if]																																						[comment CLOSE checks if the operation is split or not/]
[else]																																							[comment OPEN checks if there is not an incoming link/]
	[for (dc : DataDictionary | d.inputPort)]																													[comment OPEN read all the input datasets and store them/]
		[if (d.eContainer(Workflow).environment.oclIsKindOf(Local))]																							[comment OPEN checks if it is a local file and a container/]
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | d.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][/if][/for][if (dc.fileName.trim().indexOf('.')>0)][dc.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dc.fileName.trim()/].parquet[/if]')
	[dc.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet('[if (d.eContainer(Workflow).environment.path.startsWith('.'))][d.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][d.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | d.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dc)][st.storageName/]/[/if][/for][/let][/if][/for][if (dc.fileName.trim().indexOf('.')>0)][dc.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dc.fileName.trim()/].parquet[/if]')
		[/if]																																					[comment CLOSE checks if it is a local file and a container or a local environment/]
	[/for]																																						[comment CLOSE read all the input datasets and store them/]
[/if]																																							[comment CLOSE checks if there is an incoming link or not/]
	[generateCallTransformation(d, d)/]
[if (d.outgoing<>null)]																																			[comment OPEN checks if the DataProcessing has an outgoing link (it's not the last)/]
	[let nextDp : DataProcessing = d.outgoing.target.oclAsType(DataProcessing)]																					[comment OPEN assigns the next DataProcessing to a variable/]
[callTransformationsRecursiveTemplate(nextDp)/]
	[/let]																																						[comment OPEN assigns the next DataProcessing to a variable/]
[/if]																																							[comment CLOSE checks if the DataProcessing has an outgoing link (it's not the last)/]
[/template]


[template public generatePMML(pmml : PMMLModel)]
[comment]FALTA POR AADIR LA CLASE QUE TIENE LAS FUNCIONES DEL PMML. (HAY QUE CREARLO EN UN TEMPLATE INDEPENDIENTE, COMO EL fileFormatting)[/comment]
[comment]CAMBIAR EL FIRST POR LOS FOR COMO EN LOS RECURSIVE TEMPLATES[/comment]
pmml_model = PMMLModel(input_dataset_filepath="[pmml.inputPort->first().name.replaceAll('[(),\\s]+', '_')/]", output_dataset_filepath="data", model_learner_pmml_filepath="[pmml.filePath/], export_only_predictions=False, export_test_metrics=True[if (pmml.test<>null)], train_split=[pmml.test.trainSize/], test_split=[pmml.test.testSize/][/if])
[if (pmml.test<>null)]
pmml_model.train_and_validate_model()
[/if]
[/template]


[template public generateCallTransformation(dataProcessing : DataProcessing, dw_modified : DataProcessing)]
[comment]The code below generates the mapping transformations[/comment]
[if (dataProcessing.parameter->notEmpty())]																														[comment OPEN checks that there are parameters in the DataProcessing/]
[if (dataProcessing.dataProcessingDefinition.name='mapping')]																									[comment OPEN checks that the DataProcessing is a Mapping (FixValue-FixValue)/]
	[for (dd_in : DataDictionary | dataProcessing.inputPort)]																									[comment OPEN traverse all the input DataDictionaries (in theory there is only 1)/]
		[if dd_in.datafield->notEmpty()]																														[comment OPEN checks that there is at least one DataField/]
			[for (df : DataField | dd_in.datafield)]																											[comment OPEN traverse all the DataFields in the input DataDictionary (the same as in the output DataDictionary)/]
input_values_list=['['/][for (aParameter : Parameter | dataProcessing.parameter)  separator(', ')][if (aParameter.oclIsTypeOf(Map))][let map : Map = aParameter.oclAsType(Map)][if (df.dataType=DataType::String or df.dataType=DataType::DateTime or df.dataType=DataType::Time)]'[map.inValue/]'[else][map.inValue/][/if][/let][/if][/for][']'/]
output_values_list=['['/][for (aParameter : Parameter | dataProcessing.parameter)  separator(', ')][if (aParameter.oclIsTypeOf(Map))][let map : Map = aParameter.oclAsType(Map)][if (df.dataType=DataType::String or df.dataType=DataType::DateTime or df.dataType=DataType::Time)]'[map.outvalue/]'[else][map.outvalue/][/if][/let][/if][/for][']'/]
data_type_input_list=['['/][for (aParameter : Parameter | dataProcessing.parameter) separator(', ')][if (aParameter.oclIsTypeOf(Map))][let map : Map = aParameter.oclAsType(Map)][if (df.dataType=DataType::String)]DataType(0)[elseif (df.dataType=DataType::Time)]DataType(1)[elseif (df.dataType=DataType::Integer)]DataType(2)[elseif (df.dataType=DataType::DateTime)]DataType(3)[elseif (df.dataType=DataType::Boolean)]DataType(4)[elseif (df.dataType=DataType::Double)]DataType(5)[elseif (df.dataType=DataType::Float)]DataType(6)[else]None[/if][/let][/if][/for][']'/]
data_type_output_list=['['/][for (aParameter : Parameter | dataProcessing.parameter) separator(', ')][if (aParameter.oclIsTypeOf(Map))][let map : Map = aParameter.oclAsType(Map)][if (df.dataType=DataType::String)]DataType(0)[elseif (df.dataType=DataType::Time)]DataType(1)[elseif (df.dataType=DataType::Integer)]DataType(2)[elseif (df.dataType=DataType::DateTime)]DataType(3)[elseif (df.dataType=DataType::Boolean)]DataType(4)[elseif (df.dataType=DataType::Double)]DataType(5)[elseif (df.dataType=DataType::Float)]DataType(6)[else]None[/if][/let][/if][/for][']'/]


[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=data_transformations.transform_fix_value_fix_value(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_df, input_values_list=input_values_list,
															  output_values_list=output_values_list,
						                                      data_type_input_list = data_type_input_list,
						                                      data_type_output_list = data_type_output_list, field_in = '[df.displayName/]', field_out = '[df.out.displayName/]')

				[if (dw_modified.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
					[let dpd : Library::DataProcessingDefinition = dw_modified.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
						[if (dpd.oclIsKindOf(Library::Transformation))]
							[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
								[if (dataProcessing.eContainer(Workflow).environment.oclIsKindOf(Local))]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet('[if (dataProcessing.eContainer(Workflow).environment.path.startsWith('.'))][dataProcessing.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][dataProcessing.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | dataProcessing.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.out.fileName.trim().indexOf('.')>0)][dd_in.out.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.out.fileName.trim()/].parquet[/if]')
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (dataProcessing.eContainer(Workflow).environment.path.startsWith('.'))][dataProcessing.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][dataProcessing.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | dataProcessing.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.out.fileName.trim().indexOf('.')>0)][dd_in.out.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.out.fileName.trim()/].parquet[/if]')
								[/if]
							[/let]
						[elseif (dpd.oclIsKindOf(Library::Job))]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet([dd_in.out.name.replaceAll('[(),\\s]+', '_')/])	
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet([dd_in.out.name.replaceAll('[(),\\s]+', '_')/])
						[/if]
					[/let]
				[/if]
			[/for]																																				[comment CLOSE traverse all the DataFields in the input DataDictionary (the same as in the output DataDictionary)/]
		[else]																																					[comment OPEN checks that there are no DataField/]
input_values_list=['['/][for (aParameter : Parameter | dataProcessing.parameter)  separator(', ')][if (aParameter.oclIsTypeOf(Map))][let map : Map = aParameter.oclAsType(Map)]'[map.inValue/]'[/let][/if][/for][']'/]
output_values_list=['['/][for (aParameter : Parameter | dataProcessing.parameter)  separator(', ')][if (aParameter.oclIsTypeOf(Map))][let map : Map = aParameter.oclAsType(Map)]'[map.outvalue/]'[/let][/if][/for][']'/]

[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=data_transformations.transform_fix_value_fix_value(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_df, input_values_list=input_values_list,
															  output_values_list=output_values_list,
						                                      data_type_input_list = None,
						                                      data_type_output_list = None, field_in = None, filed_out = None)
			[if (dw_modified.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
				[let dpd : Library::DataProcessingDefinition = dw_modified.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
					[if (dpd.oclIsKindOf(Library::Transformation))]
							[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
								[if (dataProcessing.eContainer(Workflow).environment.oclIsKindOf(Local))]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet('[if (dataProcessing.eContainer(Workflow).environment.path.startsWith('.'))][dataProcessing.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][dataProcessing.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | dataProcessing.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.out.fileName.trim().indexOf('.')>0)][dd_in.out.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.out.fileName.trim()/].parquet[/if]')
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (dataProcessing.eContainer(Workflow).environment.path.startsWith('.'))][dataProcessing.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][dataProcessing.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | dataProcessing.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.out.fileName.trim().indexOf('.')>0)][dd_in.out.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.out.fileName.trim()/].parquet[/if]')
								[/if]
							[/let]
					[elseif (dpd.oclIsKindOf(Library::Job))]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet([dd_in.out.name.replaceAll('[(),\\s]+', '_')/])	
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet([dd_in.out.name.replaceAll('[(),\\s]+', '_')/])
					[/if]
				[/let]
			[/if]			
		[/if]																																					[comment CLOSE checks that there are DataFields or not/]
	[/for]																																						[comment CLOSE traverse all the input DataDictionaries (in theory there is only 1)/]
[/if]																																							[comment CLOSE checks that the DataProcessing is a Mapping (FixValue-FixValue)/]
[comment]The code below generates the transformations different from mapping for every field[/comment]
[if (dataProcessing.dataProcessingDefinition.name<>'mapping')]																									[comment OPEN checks that the DataProcessing is not a Mapping/]
[for (dd_in : DataDictionary | dataProcessing.inputPort)]																										[comment OPEN traverse all the input DataDictionaries/]
[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_df.copy()
[/for]																																							[comment CLOSE traverse all the input DataDictionaries/]
[for (p : Parameter | dataProcessing.parameter)]																												[comment OPEN traverse all the parameters in the DataProcessing/]
    [if (p.oclIsKindOf(ImputeType))]																															[comment OPEN checks that the parameter is an ImputeType/]
        [let imType : ImputeType = p.oclAsType(ImputeType)]																										[comment OPEN assigns the parameter to be an ImputeType/]
			[for (dd_in : DataDictionary | dataProcessing.inputPort)]																							[comment OPEN traverse all the input DataDictionaries (in theory there is only 1)/]
				[if dd_in.datafield->notEmpty()]																												[comment OPEN checks that there is at least DataField/]
					[for (df : DataField | dd_in.datafield)]																									[comment OPEN traverse all the DataFields in the input DataDictionary (the same as in the output DataDictionary)/]
missing_values_list=['['/][for (mv : ValueField | df.missingValues) separator(', ')][if (df.dataType=DataType::String or df.dataType=DataType::DateTime or df.dataType=DataType::Time)]'[mv.value/]'[else][mv.value/][/if][/for][']'/]

						[if (dataProcessing.dataProcessingDefinition.name='imputeByFixValue')]																	[comment OPEN checks that the DataProcessing is an imputeByFixValue(SpecialValue_FixValue)/]
			            	[if (imType.oclIsKindOf(FixValue))]																									[comment OPEN checks that the ImputeType is a FixValue/]
        	    				[let fv : FixValue = imType.oclAsType(FixValue)]																				[comment OPEN assigns the ImputeType as a FixValue/]
[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_special_value_fix_value(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
															  special_type_input=SpecialType([if (fv.imputeValue=SpecialValue::Missing)]0[elseif (fv.imputeValue=SpecialValue::Invalid)]1[elseif (fv.imputeValue=SpecialValue::Outlier)]2[/if]), fix_value_output=[if (df.dataType=DataType::String or df.dataType=DataType::DateTime or df.dataType=DataType::Time)]'[fv.value/]'[else][fv.value/][/if],
															  missing_values=missing_values_list,		
						                                      data_type_output = [if (df.dataType=DataType::String)]DataType(0)[elseif (df.dataType=DataType::Time)]DataType(1)[elseif (df.dataType=DataType::Integer)]DataType(2)[elseif (df.dataType=DataType::DateTime)]DataType(3)[elseif (df.dataType=DataType::Boolean)]DataType(4)[elseif (df.dataType=DataType::Double)]DataType(5)[elseif (df.dataType=DataType::Float)]DataType(6)[else]None[/if],
															  axis_param=0, field_in = '[df.displayName/]', field_out = '[df.out.displayName/]')

		                		[/let]																															[comment CLOSE assigns the ImputeType as a FixValue/]
		            		[/if]																																[comment CLOSE checks that the ImputeType is a FixValue/]
						[/if]																																	[comment CLOSE checks that the DataProcessing is an imputeByFixValue(SpecialValue_FixValue)/]
						[if (dataProcessing.dataProcessingDefinition.name='imputeByDerivedValue')]																[comment OPEN checks that the DataProcessing is an imputeByDerivedValue(SpecialValue_DerivedValue)/]
        					[if (imType.oclIsKindOf(DerivedValue))]																								[comment OPEN checks that the ImputeType is a DerivedValue/]
            					[let dv : DerivedValue = imType.oclAsType(DerivedValue)]																		[comment OPEN assigns the ImputeType as a DerivedValue/]
[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_special_value_derived_value(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
															  special_type_input=SpecialType([if (dv.imputeValue=SpecialValue::Missing)]0[elseif (dv.imputeValue=SpecialValue::Invalid)]1[elseif (dv.imputeValue=SpecialValue::Outlier)]2[/if]), derived_type_output=DerivedType([if (dv.type=DerivedType::MostFrequent)]0[elseif (dv.type=DerivedType::Previous)]1[elseif (dv.type=DerivedType::Next)]2[/if]),
															  missing_values=missing_values_list,		
															  axis_param=0, field_in = '[df.displayName/]', field_out = '[df.out.displayName/]')

		                		[/let]																															[comment CLOSE assigns the ImputeType as a DerivedValue/]
		            		[/if]																																[comment CLOSE checks that the ImputeType is a DerivedValue/]
						[/if]																																	[comment CLOSE checks that the DataProcessing is an imputeByDerivedValue(SpecialValue_DerivedValue)/]
						[if (dataProcessing.dataProcessingDefinition.name='imputeByNumericOp')]																	[comment OPEN checks that the DataProcessing is an imputeByNumericOp(SpecialValue_NumOp)/]
        					[if (imType.oclIsKindOf(NumOp))]																									[comment OPEN checks that the ImputeType is a NumOp/]
            					[let nop : NumOp = imType.oclAsType(NumOp)]																						[comment OPEN assigns the ImputeType as a NumOp/]
[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_special_value_num_op(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
															  special_type_input=SpecialType([if (nop.imputeValue=SpecialValue::Missing)]0[elseif (nop.imputeValue=SpecialValue::Invalid)]1[elseif (nop.imputeValue=SpecialValue::Outlier)]2[/if]), num_op_output=Operation([if (nop.operation=Operation::Interpolation)]0[elseif (nop.operation=Operation::Mean)]1[elseif (nop.operation=Operation::Median)]2[elseif (nop.operation=Operation::Closest)]3[/if]),
															  missing_values=missing_values_list,		
															  axis_param=0, field_in = '[df.displayName/]', field_out = '[df.out.displayName/]')

			                	[/let]																															[comment CLOSE assigns the ImputeType as a NumOp/]
			            	[/if]																																[comment CLOSE checks that the ImputeType is a NumOp/]
						[/if]																																	[comment CLOSE checks that the DataProcessing is an imputeByNumericOp(SpecialValue_NumOp)/]
					[/for]																																		[comment CLOSE traverse all the DataFields in the input DataDictionary (the same as in the output DataDictionary)/]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed
					[if (dw_modified.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
						[let dpd : Library::DataProcessingDefinition = dw_modified.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
							[if (dpd.oclIsKindOf(Library::Transformation))]
								[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
									[if (dataProcessing.eContainer(Workflow).environment.oclIsKindOf(Local))]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet('[if (dataProcessing.eContainer(Workflow).environment.path.startsWith('.'))][dataProcessing.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][dataProcessing.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | dataProcessing.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.out.fileName.trim().indexOf('.')>0)][dd_in.out.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.out.fileName.trim()/].parquet[/if]')
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (dataProcessing.eContainer(Workflow).environment.path.startsWith('.'))][dataProcessing.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][dataProcessing.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | dataProcessing.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.out.fileName.trim().indexOf('.')>0)][dd_in.out.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.out.fileName.trim()/].parquet[/if]')
									[/if]
								[/let]
							[elseif (dpd.oclIsKindOf(Library::Job))]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet([dd_in.out.name.replaceAll('[(),\\s]+', '_')/])	
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet([dd_in.out.name.replaceAll('[(),\\s]+', '_')/])
							[/if]
						[/let]
					[/if]
[comment]The code below generates the same transformation generated above but for the whole dataframe instead of a field[/comment]
				[else]																																			[comment OPEN checks that there are no DataFields/]
					[if (dataProcessing.dataProcessingDefinition.name='imputeByFixValue')]																		[comment OPEN checks that the DataProcessing is an imputeByFixValue(SpecialValue_FixValue)/]
						[if (imType.oclIsKindOf(FixValue))]																										[comment OPEN checks that the ImputeType is a FixValue/]
		            	[let fv : FixValue = imType.oclAsType(FixValue)]																						[comment OPEN assigns the ImputeType as a FixValue/]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=data_transformations.transform_special_value_fix_value(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_df,
															  special_type_input=SpecialType([if (imType.imputeValue=SpecialValue::Missing)]0[elseif (imType.imputeValue=SpecialValue::Invalid)]1[elseif (imType.imputeValue=SpecialValue::Outlier)]2[/if]), fix_value_output='[fv.value/]', missing_values=None
						                                      data_type_output = None, axis_param=0, field_in = None, field_out = None)		

		            		[/let]																																[comment CLOSE assigns the ImputeType as a FixValue/]
		        		[/if]																																	[comment CLOSE checks that the ImputeType is a FixValue/]
					[/if]																																		[comment CLOSE checks that the DataProcessing is an imputeByFixValue(SpecialValue_FixValue)/]
					[if (dataProcessing.dataProcessingDefinition.name='imputeByDerivedValue')]																	[comment OPEN checks that the DataProcessing is an imputeByDerivedValue(SpecialValue_DerivedValue)/]	
        				[if (imType.oclIsKindOf(DerivedValue))]																									[comment OPEN checks that the ImputeType is a DerivedValue/]
            				[let dv : DerivedValue = imType.oclAsType(DerivedValue)]																			[comment OPEN assigns the ImputeType as a DerivedValue/]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=data_transformations.transform_special_value_derived_value(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_df,
															  special_type_input=SpecialType([if (dv.imputeValue=SpecialValue::Missing)]0[elseif (dv.imputeValue=SpecialValue::Invalid)]1[elseif (dv.imputeValue=SpecialValue::Outlier)]2[/if]), derived_type_output=DerivedType([if (dv.type=DerivedType::MostFrequent)]0[elseif (dv.type=DerivedType::Previous)]1[elseif (dv.type=DerivedType::Next)]2[/if]),
															  missing_values=None, axis_param=0, field_in = None, field_out = None)		

			            	[/let]																																[comment CLOSE assigns the ImputeType as a DerivedValue/]
			        	[/if]																																	[comment CLOSE checks that the ImputeType is a DerivedValue/]
					[/if]																																		[comment CLOSE checks that the DataProcessing is an imputeByDerivedValue(SpecialValue_DerivedValue)/]
					[if (dataProcessing.dataProcessingDefinition.name='imputeByNumericOp')]																		[comment OPEN checks that the DataProcessing is an imputeByNumericOp(SpecialValue_NumOp)/]
    					[if (imType.oclIsKindOf(NumOp))]																										[comment OPEN checks that the ImputeType is a NumOp/]
            				[let nop : NumOp = imType.oclAsType(NumOp)]																							[comment OPEN assigns the ImputeType as a NumOp/]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=data_transformations.transform_special_value_num_op(data_dictionary=[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df,
															  special_type_input=SpecialType([if (nop.imputeValue=SpecialValue::Missing)]0[elseif (nop.imputeValue=SpecialValue::Invalid)]1[elseif (nop.imputeValue=SpecialValue::Outlier)]2[/if]), num_op_output=Operation([if (nop.operation=Operation::Interpolation)]0[elseif (nop.operation=Operation::Mean)]1[elseif (nop.operation=Operation::Median)]2[elseif (nop.operation=Operation::Closest)]3[/if]),
															  missing_values=None, axis_param=0, field_in = None, field_out = None)		

			            	[/let]																																[comment CLOSE assigns the ImputeType as a NumOp/]
			        	[/if]																																	[comment CLOSE checks that the ImputeType is a NumOp/]
					[/if]																																		[comment CLOSE checks that the DataProcessing is an imputeByNumericOp(SpecialValue_NumOp)/]
					[if (dw_modified.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
						[let dpd : Library::DataProcessingDefinition = dw_modified.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
							[if (dpd.oclIsKindOf(Library::Transformation))]
								[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
									[if (dataProcessing.eContainer(Workflow).environment.oclIsKindOf(Local))]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet('[if (dataProcessing.eContainer(Workflow).environment.path.startsWith('.'))][dataProcessing.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][dataProcessing.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | dataProcessing.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.out.fileName.trim().indexOf('.')>0)][dd_in.out.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.out.fileName.trim()/].parquet[/if]')
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (dataProcessing.eContainer(Workflow).environment.path.startsWith('.'))][dataProcessing.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][dataProcessing.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | dataProcessing.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.out.fileName.trim().indexOf('.')>0)][dd_in.out.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.out.fileName.trim()/].parquet[/if]')
									[/if]
								[/let]
							[elseif (dpd.oclIsKindOf(Library::Job))]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet([dd_in.out.name.replaceAll('[(),\\s]+', '_')/])	
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet([dd_in.out.name.replaceAll('[(),\\s]+', '_')/])
							[/if]
						[/let]
					[/if]
				[/if]																																			[comment CLOSE checks if there are DataFields or not/]
			[/for]																																				[comment CLOSE traverse all the input DataDictionaries (in theory there is only 1)/]
		[/let]																																					[comment CLOSE assigns the parameter to be an ImputeType/]
	[/if]																																						[comment CLOSE checks that the parameter is an ImputeType/]
	[if (dataProcessing.dataProcessingDefinition.name='binner')]																								[comment OPEN checks that the DataProcessing is a Binner (interval_FixValue)/]
		[if (p.oclIsKindOf(DiscretizeBin))]																														[comment OPEN checks that the parameter is an DiscretizeBin/]
			[let binner : DiscretizeBin = p.oclAsType(DiscretizeBin)]																							[comment OPEN assigns the parameter to be a DiscretizeBin/]
				[for (dd_in : DataDictionary | dataProcessing.inputPort)]																						[comment OPEN traverse all the input DataDictionaries (in theory there is only 1)/]
					[if dd_in.datafield->notEmpty()]																											[comment OPEN checks that there is at least DataField/]
						[for (df : DataField | dd_in.datafield)]																								[comment OPEN traverse all the DataFields in the input DataDictionary/]
							[for (interval : Interval | binner.interval)]																						[comment OPEN traverse all the intervals in the binner/]
[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_interval_fix_value(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
															  left_margin=[interval.leftMargin/], right_margin=[interval.rightMargin/],
															  closure_type=Closure([if (interval.clousure=ClosureType::opeOpen)]0[elseif(interval.clousure=ClosureType::openClosed)]1[elseif(interval.clousure=ClosureType::closedOpen)]2[elseif(interval.clousure=ClosureType::closedClosed)]3[/if]),
															  fix_value_output=[if (df.out.dataType=DataType::String or df.out.dataType=DataType::DateTime or df.out.dataType=DataType::Time)]'[binner.binValue/]'[else][binner.binValue/][/if],
						                                      data_type_output = [if (df.out.dataType=DataType::String)]DataType(0)[elseif (df.out.dataType=DataType::Time)]DataType(1)[elseif (df.out.dataType=DataType::Integer)]DataType(2)[elseif (df.out.dataType=DataType::DateTime)]DataType(3)[elseif (df.out.dataType=DataType::Boolean)]DataType(4)[elseif (df.out.dataType=DataType::Double)]DataType(5)[elseif (df.out.dataType=DataType::Float)]DataType(6)[else]None[/if],
															  field_in = '[df.displayName/]',
															  field_out = '[df.out.displayName/]')

							[/for]																																[comment CLOSE traverse all the intervals in the binner/]
						[/for]																																	[comment CLOSE traverse all the DataFields in the input DataDictionary/]
					[else]																																		[comment OPEN checks that there are not DataFields/]
						[for (interval : Interval | binner.interval)]																							[comment OPEN traverse all the intervals in the binner/]
[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_interval_fix_value(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
														  left_margin=[interval.leftMargin/], right_margin=[interval.rightMargin/],
														  closure_type=Closure([if (interval.clousure=ClosureType::opeOpen)]0[elseif(interval.clousure=ClosureType::openClosed)]1[elseif(interval.clousure=ClosureType::closedOpen)]2[elseif(interval.clousure=ClosureType::closedClosed)]3[/if]),
														  fix_value_output='[binner.binValue/]', data_type_output = None, field_in = None, field_out = None)

						[/for]																																	[comment CLOSE traverse all the intervals in the binner/]
					[/if]																																		[comment CLOSE checks if there are DataFields/]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed
					[if (dw_modified.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
						[let dpd : Library::DataProcessingDefinition = dw_modified.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
							[if (dpd.oclIsKindOf(Library::Transformation))]
								[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
									[if (dataProcessing.eContainer(Workflow).environment.oclIsKindOf(Local))]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet('[if (dataProcessing.eContainer(Workflow).environment.path.startsWith('.'))][dataProcessing.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][dataProcessing.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | dataProcessing.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.out.fileName.trim().indexOf('.')>0)][dd_in.out.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.out.fileName.trim()/].parquet[/if]')
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (dataProcessing.eContainer(Workflow).environment.path.startsWith('.'))][dataProcessing.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][dataProcessing.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | dataProcessing.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.out.fileName.trim().indexOf('.')>0)][dd_in.out.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.out.fileName.trim()/].parquet[/if]')
									[/if]
								[/let]
							[elseif (dpd.oclIsKindOf(Library::Job))]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet([dd_in.out.name.replaceAll('[(),\\s]+', '_')/])	
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet([dd_in.out.name.replaceAll('[(),\\s]+', '_')/])
							[/if]
						[/let]
					[/if]
				[/for]																																			[comment CLOSE traverse all the intervals in the binner/]
			[/let]																																				[comment OPEN assigns the parameter to be an DiscretizeBin/]
		[/if]																																					[comment CLOSE checks that the parameter is an DiscretizeBin/]
	[/if]																																						[comment CLOSE checks that the DataProcessing is a Binner (interval_FixValue)/]
	[if (p.oclIsKindOf(DerivedField))]																															[comment OPEN checks that the parameter is a DerivedField/]
		[let derf : DerivedField = p.oclAsType(DerivedField)]																									[comment OPEN assigns the parameter to be a DerivedField/]
			[for (dd_in : DataDictionary | dataProcessing.inputPort)]																							[comment OPEN traverse al the input DataDictionaries/]
[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_derived_field(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
															  data_type_output = [if (derf.der_dataField.dataType=DataType::String)]DataType(0)[elseif (derf.der_dataField.dataType=DataType::Time)]DataType(1)[elseif (derf.der_dataField.dataType=DataType::Integer)]DataType(2)[elseif (derf.der_dataField.dataType=DataType::DateTime)]DataType(3)[elseif (derf.der_dataField.dataType=DataType::Boolean)]DataType(4)[elseif (derf.der_dataField.dataType=DataType::Double)]DataType(5)[elseif (derf.der_dataField.dataType=DataType::Float)]DataType(6)[else]None[/if],
															  field_in = '[derf.der_dataField._in.displayName/]', field_out = '[derf.der_dataField.displayName/]')

[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed
				[if (dw_modified.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
					[let dpd : Library::DataProcessingDefinition = dw_modified.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
						[if (dpd.oclIsKindOf(Library::Transformation))]
							[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
								[if (dataProcessing.eContainer(Workflow).environment.oclIsKindOf(Local))]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet('[if (dataProcessing.eContainer(Workflow).environment.path.startsWith('.'))][dataProcessing.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][dataProcessing.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | dataProcessing.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.out.fileName.trim().indexOf('.')>0)][dd_in.out.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.out.fileName.trim()/].parquet[/if]')
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (dataProcessing.eContainer(Workflow).environment.path.startsWith('.'))][dataProcessing.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][dataProcessing.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | dataProcessing.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.out.fileName.trim().indexOf('.')>0)][dd_in.out.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.out.fileName.trim()/].parquet[/if]')
								[/if]
							[/let]
						[elseif (dpd.oclIsKindOf(Library::Job))]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet([dd_in.out.name.replaceAll('[(),\\s]+', '_')/])	
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet([dd_in.out.name.replaceAll('[(),\\s]+', '_')/])
						[/if]
					[/let]
				[/if]
			[/for]																																				[comment CLOSE traverse all the input DataDictionaries/]
		[/let]																																					[comment CLOSE assigns the parameter to be a DerivedField/]
	[/if]																																						[comment CLOSE checks that the parameter is a DerivedField/]
	[if (dataProcessing.dataProcessingDefinition.name='categoricalToContinuous')]																				[comment OPEN checks that the DataProcessing is categoricalToContinuous(StringToNumber)/]
		[if (p.oclIsKindOf(CastType))]																															[comment OPEN checks that the parameter is a CastType/]
			[let cast : CastType = p.oclAsType(CastType)]																										[comment OPEN assigns the variable to be a CastType/]
				[for (dd_in : DataDictionary | dataProcessing.inputPort)]																						[comment OPEN traverse the input DataDictionaries/]
					[for (df : DataField | dd_in.datafield)]																									[comment OPEN traverse the DataFields/]
[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_cast_type(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
																data_type_output= [if (cast.type=DataType::String)]DataType(0)[elseif (cast.type=DataType::Time)]DataType(1)[elseif (cast.type=DataType::Integer)]DataType(2)[elseif (cast.type=DataType::DateTime)]DataType(3)[elseif (cast.type=DataType::Boolean)]DataType(4)[elseif (cast.type=DataType::Double)]DataType(5)[elseif (cast.type=DataType::Float)]DataType(6)[else]None[/if],
																field='[df.displayName/]')

					[/for]																																		[comment CLOSE traverse the DataFields/]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed
					[if (dw_modified.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
						[let dpd : Library::DataProcessingDefinition = dw_modified.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
							[if (dpd.oclIsKindOf(Library::Transformation))]
								[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
									[if (dataProcessing.eContainer(Workflow).environment.oclIsKindOf(Local))]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet('[if (dataProcessing.eContainer(Workflow).environment.path.startsWith('.'))][dataProcessing.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][dataProcessing.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | dataProcessing.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.out.fileName.trim().indexOf('.')>0)][dd_in.out.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.out.fileName.trim()/].parquet[/if]')
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (dataProcessing.eContainer(Workflow).environment.path.startsWith('.'))][dataProcessing.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][dataProcessing.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | dataProcessing.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.out.fileName.trim().indexOf('.')>0)][dd_in.out.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.out.fileName.trim()/].parquet[/if]')
									[/if]
								[/let]
							[elseif (dpd.oclIsKindOf(Library::Job))]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet([dd_in.out.name.replaceAll('[(),\\s]+', '_')/])	
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet([dd_in.out.name.replaceAll('[(),\\s]+', '_')/])
							[/if]
						[/let]
					[/if]
				[/for]																																			[comment CLOSE traverse the input DataDictionaries/]
			[/let]																																				[comment CLOSE assigns the variable to be a CastType/]
		[/if]																																					[comment CLOSE checks that the parameter is a CastType/]
	[/if]																																						[comment CLOSE checks that the DataProcessing is categoricalToContinuous(StringToNumber)/]
	[if (dataProcessing.dataProcessingDefinition.name='rowFilter' or dataProcessing.dataProcessingDefinition.name='rowFilterRange')]							[comment OPEN checks that the DataProcessing is rowFilter/]
		[if (p.oclIsKindOf(FilterValue))]																														[comment OPEN checks that the parameter is a FilterValue/]
			[for (dd_in : DataDictionary | dataProcessing.inputPort)]
				[let fValue : FilterValue = p.oclAsType(FilterValue)]																							[comment OPEN assigns the variable to be a FilterValue/]
columns_[fValue.filterValueDef.name/]=['['/][for (df : DataField | dd_in.datafield) separator (', ')]'[df.displayName/]'[/for][']'/]

					[if (not fValue.primitive->isEmpty())]																										[comment OPEN checks that the list of primitives is not empty/]
filter_fix_value_list_[fValue.filterValueDef.name/]=['['/][for (prim : Primitive | fValue.primitive) separator (', ')][prim.value/][/for][']'/]

[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_filter_rows_primitive(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
																										columns=columns_[fValue.filterValueDef.name/],
																	                                    filter_fix_value_list=filter_fix_value_list_[fValue.filterValueDef.name/],
																										filter_type=FilterType([if (fValue.filterType=FilterType::EXCLUDE)]0[else]1[/if]))
					[/if]																																		[comment CLOSE checks that the list of primitives is not empty/]
					[if (not fValue.matchingvalue->isEmpty())]																									[comment OPEN checks that the list of MatchingValues is not empty/]
						[let filteredSet : OrderedSet(SpecialValues) = fValue.matchingvalue->select(e | e.oclIsKindOf(SpecialValues))]							[comment OPEN assigns to a set only the SpecialValues/]
							[if (not filteredSet->isEmpty())]																									[comment OPEN checks that list of SpecialValues is not empty/]
dicc_[fValue.filterValueDef.name/]={[for (df : DataField | dd_in.datafield) separator (', ')]'[df.displayName/]':{[for (spVal : SpecialValues | filteredSet) separator (', ')][if (spVal.specialType=SpecialValue::Missing)]'missing': ['['/][for (mv : ValueField | df.missingValues) separator(', ')][if (df.dataType=DataType::String or df.dataType=DataType::Time or df.dataType=DataType::DateTime)]'[mv.value/]'[else][mv.value/][/if][/for][']'/][elseif(spVal.specialType=SpecialValue::Invalid)]'invalid': ['['/][for (iv : ValueField | df.invalidValues) separator(', ')][if (df.dataType=DataType::String or df.dataType=DataType::Time or df.dataType=DataType::DateTime)]'[iv.value/]'[else][iv.value/][/if][/for][']'/] [elseif(spVal.specialType=SpecialValue::Outlier)]'outlier': True[/if][/for]}[/for]}

[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_filter_rows_special_values(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
																										cols_special_type_values=dicc_[fValue.filterValueDef.name/],
																										filter_type=FilterType([if (fValue.filterType=FilterType::EXCLUDE)]0[else]1[/if]))
							[/if]																																[comment CLOSE checks that list of SpecialValues is not empty/]
						[/let]																																	[comment CLOSE assigns to a set only the SpecialValues/]
						[let filteredSet : OrderedSet(Range) = fValue.matchingvalue->select(e | e.oclIsKindOf(Range))]											[comment OPEN assigns to a set only the Range/]
							[if (not filteredSet->isEmpty())]																									[comment OPEN checks that the list of Range is not empty/]
filter_range_left_values_list_[fValue.filterValueDef.name/]=['['/][for (range : Range | filteredSet) separator(', ')][if (range.minInfinity=true)]-np.inf[else][range.min/][/if][/for][']'/]
filter_range_right_values_list_[fValue.filterValueDef.name/]=['['/][for (range : Range | filteredSet) separator(', ')][if (range.maxInfinity=true)]np.inf[else][range.max/][/if][/for][']'/]
closure_type_list_[fValue.filterValueDef.name/]=['['/][for (range : Range | filteredSet) separator(', ')]Closure([if (range.clousure=ClosureType::opeOpen)]0[elseif(range.clousure=ClosureType::openClosed)]1[elseif(range.clousure=ClosureType::closedOpen)]2[elseif(range.clousure=ClosureType::closedClosed)]3[/if])[/for][']'/]

[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_filter_rows_range(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
																										columns=columns_[fValue.filterValueDef.name/],
																										left_margin_list=filter_range_left_values_list_[fValue.filterValueDef.name/],
																										right_margin_list=filter_range_right_values_list_[fValue.filterValueDef.name/],
																										filter_type=FilterType([if (fValue.filterType=FilterType::EXCLUDE)]0[else]1[/if]),
																										closure_type_list=closure_type_list_[fValue.filterValueDef.name/])
							[/if]																																[comment CLOSE checks that the list of Range is not empty/]
						[/let]																																	[comment CLOSE assigns to a set only the Range/]
					[/if]																																		[comment CLOSE checks that the list of MatchingValues is not empty/]
				[/let]	
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed
				[if (dw_modified.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
					[let dpd : Library::DataProcessingDefinition = dw_modified.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]
						[if (dpd.oclIsKindOf(Library::Transformation))]
							[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
								[if (dataProcessing.eContainer(Workflow).environment.oclIsKindOf(Local))]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet('[if (dataProcessing.eContainer(Workflow).environment.path.startsWith('.'))][dataProcessing.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][dataProcessing.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | dataProcessing.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.out.fileName.trim().indexOf('.')>0)][dd_in.out.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.out.fileName.trim()/].parquet[/if]')
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (dataProcessing.eContainer(Workflow).environment.path.startsWith('.'))][dataProcessing.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][dataProcessing.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | dataProcessing.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.out.fileName.trim().indexOf('.')>0)][dd_in.out.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.out.fileName.trim()/].parquet[/if]')
								[/if]
							[/let]
						[elseif (dpd.oclIsKindOf(Library::Job))]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet([dd_in.out.name.replaceAll('[(),\\s]+', '_')/])	
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet([dd_in.out.name.replaceAll('[(),\\s]+', '_')/])
						[/if]
					[/let]
				[/if]
			[/for]																																				[comment CLOSE traverse all the input DataDictionaries/]
		[/if]																																					[comment CLOSE checks that the parameter is a FilterValue/]
	[/if]																																						[comment CLOSE checks that the DataProcessing is rowFilter/]
	[if (dataProcessing.dataProcessingDefinition.name='columnFilter')]																							[comment OPEN checks that the DataProcessing is columnFilter/]
		[if (p.oclIsKindOf(Field))]																																[comment OPEN checks that the parameter is a Field/]
			[let field : Field = p.oclAsType(Field)]																											[comment OPEN assigns the variable to be a field/]
				[for (dd_in : DataDictionary | dataProcessing.inputPort)]																						[comment OPEN traverse all the input DataDictionaries/]
field_list_[field.fieldDef.name/]=['['/][for (df : DataField | field.dataField) separator (', ')]'[df.displayName/]'[/for][']'/]

[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed=data_transformations.transform_filter_columns(data_dictionary=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed,
																columns=field_list_[field.fieldDef.name/], belong_op=Belong.[field.operator/])

[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=[dd_in.name.replaceAll('[(),\\s]+', '_')/]_transformed
					[if (dw_modified.dataProcessingDefinition.oclIsKindOf(Library::DataProcessingDefinition))]
						[let dpd : Library::DataProcessingDefinition = dw_modified.dataProcessingDefinition.oclAsType(Library::DataProcessingDefinition)]		
							[if (dpd.oclIsKindOf(Library::Transformation))]
								[let libT : Library::Transformation = dpd.oclAsType(Library::Transformation)]
									[if (dataProcessing.eContainer(Workflow).environment.oclIsKindOf(Local))]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet('[if (dataProcessing.eContainer(Workflow).environment.path.startsWith('.'))][dataProcessing.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][dataProcessing.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | dataProcessing.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.out.fileName.trim().indexOf('.')>0)][dd_in.out.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.out.fileName.trim()/].parquet[/if]')
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet('[if (dataProcessing.eContainer(Workflow).environment.path.startsWith('.'))][dataProcessing.eContainer(Workflow).environment.path.replaceFirst('./', '/')/][else][dataProcessing.eContainer(Workflow).environment.path/][/if]/data/[for (st : Storage | dataProcessing.eContainer(Workflow).environment.storage)][if (st.oclIsKindOf(Database) and st.storageName.size()>0)][let db : Database = st.oclAsType(Database)][for(tab : Table | db.table)][if(tab.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][elseif(st.oclIsKindOf(Folder) and st.storageName.size()>0)][let fo : Folder = st.oclAsType(Folder)][for(fi : File | fo.file)][if(fi.dataDictionary=dd_in.out)][st.storageName/]/[/if][/for][/let][/if][/for][if (dd_in.out.fileName.trim().indexOf('.')>0)][dd_in.out.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][dd_in.out.fileName.trim()/].parquet[/if]')
									[/if]
								[/let]
							[elseif (dpd.oclIsKindOf(Library::Job))]
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df.to_parquet([dd_in.out.name.replaceAll('[(),\\s]+', '_')/])	
[dd_in.out.name.replaceAll('[(),\\s]+', '_')/]_df=pd.read_parquet([dd_in.out.name.replaceAll('[(),\\s]+', '_')/])
							[/if]
						[/let]
					[/if]
				[/for]																																			[comment CLOSE traverse all the input DataDictionaries/]
			[/let]																																				[comment CLOSE assigns the variable to be a field/]
		[/if]																																					[comment CLOSE checks that the parameter is a Field/]
	[/if]																																						[comment CLOSE checks that the DataProcessing is columnFilter/]
[/for]																																							[comment CLOSE traverse all the parameters in the DataProcessing/]
[/if]																																							[comment CLOSE checks that the DataProcessing is not a Mapping/]
[/if]																																							[comment CLOSE checks that there are parameters in the DataProcessing/]
[/template]


[template public generateEnvironment(aWorkflow : Workflow)]
[if (aWorkflow.environment.oclIsKindOf(Local))]
[generateLocalDocker()/]
[elseif (aWorkflow.environment.oclIsKindOf(AzureCI) or aWorkflow.environment.oclIsKindOf(AmazonECS))]
[generateRemoteDocker()/]
[/if]
[/template]


[template public generateRemoteDocker(aWorkflow : Workflow)]
[if (aWorkflow.environment.oclIsTypeOf(AmazonECS))]
[generateAmazonECS()/]
[elseif (aWorkflow.environment.oclIsTypeOf(AzureCI))]
[generateAzureCI()/]
[/if]
[/template]


[template public generateLocalDocker(aWorkflow : Workflow)]
[let doc : Local = aWorkflow.environment.oclAsType(Local)]
	[file ('Dockerfile', false, 'UTF-8')]
		[if (doc.developmentTool.tool=TOOL::PYTHON)]
FROM python:[doc.developmentTool.version/]-slim
		[elseif (doc.developmentTool.tool=TOOL::KNIME)]
FROM python:3.11-slim
#FROM KNIME:[doc.developmentTool.version/]
		[elseif (doc.developmentTool.tool=TOOL::R)]
FROM python:3.11-slim
#FROM R:[doc.developmentTool.version/]
		[/if]

WORKDIR [aWorkflow.environment.path/]
ENV PYTHONPATH=[aWorkflow.environment.path/]

RUN apt-get update && \
    apt-get install -y --no-install-recommends git && \
    rm -rf /var/lib/apt/lists/*


RUN git clone --depth 1 --branch develop https://github.com/franjmelchor/MD4DSP-m2python.git .

RUN pip install --no-cache-dir -r requirements.txt
RUN pip install h5py
RUN pip install pyarrow
[let filteredSet : OrderedSet(Database) = aWorkflow.environment.storage->select(e | e.oclIsKindOf(Database))]
	[if (not filteredSet->isEmpty())]
RUN pip install sqlalchemy
		[let types : Set(Environment::DBTYPE) = filteredSet->collect(e | e.type)->asSet()]
			[for (type : DBTYPE | types)]
				[if (type=DBTYPE::MYSQL)]
RUN pip install mysql-connector-python --upgrade
				[elseif (type=DBTYPE::POSTGRESQL)]
RUN pip install psycopg2-binary
				[elseif (type=DBTYPE::SQLSERVER)]
RUN pip3 install pyodbc
RUN apt-get update && \
    apt-get install -y curl gnupg apt-transport-https unixodbc-dev && \
    curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add - && \
    curl https://packages.microsoft.com/config/ubuntu/20.04/prod.list > /etc/apt/sources.list.d/mssql-release.list && \
    apt-get update && ACCEPT_EULA=Y apt-get install -y msodbcsql17 && \
    rm -rf /var/lib/apt/lists/*
				[elseif (type=DBTYPE::ORACLE)]
RUN pip install oracledb
				[elseif (type=DBTYPE::MONGODB)]
RUN pip install pymongo
				[elseif (type=DBTYPE::DYNAMODB)]
RUN pip install boto3
				[/if]
			[/for]
		[/let]
	[/if]
[/let]

COPY dataProcessing_Job_[aWorkflow.name.replaceAll('[(),\\s]+', '_')/].py [aWorkflow.environment.path/]/workflows/
COPY contracts_Job_[aWorkflow.name.replaceAll('[(),\\s]+', '_')/].py [aWorkflow.environment.path/]/workflows/
COPY transformations_Job_[aWorkflow.name.replaceAll('[(),\\s]+', '_')/].py [aWorkflow.environment.path/]/workflows/

COPY fileFormatting.py [aWorkflow.environment.path/]/workflows/

COPY workflow_scripts.sh [aWorkflow.environment.path/]/workflow_scripts.sh

RUN chmod +x [aWorkflow.environment.path/]/workflow_scripts.sh

CMD ['['/]"/bin/bash", "[aWorkflow.environment.path/]/workflow_scripts.sh"[']'/]
	[/file]


	[file ('deploy_docker_app.sh', false, 'UTF-8')]
#!/bin/bash
set -e

if ['['/] ! -d "data" [']'/]; then
    mkdir data
fi

[for (st: Storage | doc.storage)]
if ['['/] ! -d "data/[st.storageName/]" [']'/]; then
    mkdir data/[st.storageName/]
fi
[/for]

sudo apt-get update --yes
sudo apt-get install ca-certificates curl
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc

echo \
  "deb ['['/]arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update --yes

sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin --yes

docker build -t [doc.imageName.replaceAll('[(),\\s]+', '_')/]:[doc.imageTag.replaceAll('[(),\\s]+', '_')/] -f Dockerfile .

clear

[for (st: Storage | doc.storage)]
	[if (st.oclIsKindOf(LocalFolder))]
		[let lf : LocalFolder = st.oclAsType(LocalFolder)]
			[for (fi : File | lf.file)]
				[if (fi.dataDictionary.fileName.trim().indexOf('.')>0)]
cp [lf.folderPath/]/[fi.dataDictionary.fileName.trim()/] "$(pwd)/data/"[lf.storageName/]
				[elseif (fi.type=FileType::CSV or fi.type=FileType::FEATHER or fi.type=FileType::JSON or fi.type=FileType::PARQUET)]
cp [lf.folderPath/]/[fi.dataDictionary.fileName.trim()/].[fi.type.toString().toLowerCase()/] "$(pwd)/data/"[lf.storageName/]
				[elseif (fi.type=FileType::HDF5)]
cp [lf.folderPath/]/[fi.dataDictionary.fileName.trim()/].hdf "$(pwd)/data/"[lf.storageName/]
				[elseif (fi.type=FileType::EXCEL)]
cp [lf.folderPath/]/[fi.dataDictionary.fileName.trim()/].xlsx "$(pwd)/data/"[lf.storageName/]
				[/if]
			[/for]
		[/let]
	[/if]
[/for]

docker run -it --rm --name [doc.name.replaceAll('[(),\\s]+', '_')/] --network host --mount type=bind,source="$(pwd)/data",target=[if (doc.path.startsWith('.'))][doc.path.replaceFirst('.', '')/][else][doc.path/][/if]/data [doc.imageName.replaceAll('[(),\\s]+', '_')/]:[doc.imageTag.replaceAll('[(),\\s]+', '_')/]

docker rmi [doc.imageName.replaceAll('[(),\\s]+', '_')/]:[doc.imageTag.replaceAll('[(),\\s]+', '_')/]

clear

echo -e "Exiting the application...\n"
	[/file]
[/let]


[file ('workflow_scripts.sh', false, 'UTF-8')]
CONTRACTS_SCRIPT="workflows.contracts_Job_[aWorkflow.name.replaceAll('[(),\\s]+', '_')/]"
TRANSFORMATIONS_SCRIPT="workflows.transformations_Job_[aWorkflow.name.replaceAll('[(),\\s]+', '_')/]"
WORKFLOW_SCRIPT="workflows.dataProcessing_Job_[aWorkflow.name.replaceAll('[(),\\s]+', '_')/]"

python3 -m workflows.fileFormatting


while true; do
    echo -e "\nWhat would you like to do?"
    echo "    1. Execute the Workflow validation contracts"
    echo "    2. Execute the Workflow data transformations"
    echo "    3. Execute the complete Pipeline (transformations and contracts)"
    echo -e "    4. Exit\n"

    read -r -p "Select an option: " option
    clear

	if ['['/] "$option" -eq 1 [']'/]; then
        echo -e "Executing the Workflow validation contracts...\n"
        if ! python3 -m $CONTRACTS_SCRIPT; then
            echo "An error occurred while executing the Workflow validation contracts."
        fi
    elif ['['/] "$option" -eq 2 [']'/]; then
        echo -e "Executing the Workflow data transformations...\n"
        if ! python3 -m $TRANSFORMATIONS_SCRIPT; then
            echo "An error occurred while executing the Workflow data transformations."
        fi
    elif ['['/] "$option" -eq 3 [']'/]; then
        echo -e "Executing the complete Pipeline...\n"
        if ! python3 -m $WORKFLOW_SCRIPT; then
            echo "An error occurred while executing the complete Pipeline."
        fi
    elif ['['/] "$option" -eq 4 [']'/]; then
        break
    else
        echo -e "Invalid option. Please select a valid option.\n"
    fi
done
[/file]
[/template]


[template public generateAmazonECS(aWorkflow : Workflow)]
[file ('AmazonECSfile', false, 'UTF-8')]
	[let ecs : AmazonECS = aWorkflow.environment.oclAsType(AmazonECS)]
SecretId = [ecs.secretId/]
Region = [ecs.region/]
Key = [ecs.key/]
	[/let]
[/file]


[/template]


[template public generateAzureCI(aWorkflow : Workflow)]
[file ('AzureCIfile', false, 'UTF-8')]
	[let ci : AzureCI = aWorkflow.environment.oclAsType(AzureCI)]
AcrName = [ci.acrName/]
KeyVaultName = [ci.keyVaultName/]
SecretName = [ci.secretName/]
BlobName = [ci.blobName/]
	[/let]
[/file]

[/template]


[template public fileFormatting(aWorkflow : Workflow)]
[file ('fileFormatting.py', false, 'UTF-8')]
import pandas as pd
import json
import h5py
import pyarrow
[let filteredSet : OrderedSet(Database) = aWorkflow.environment.storage->select(e | e.oclIsKindOf(Database))]
	[if (not filteredSet->isEmpty())]
from sqlalchemy import create_engine
		[let types : Set(Environment::DBTYPE) = filteredSet->collect(e | e.type)->asSet()]
			[for (type : DBTYPE | types)]
				[if (type=DBTYPE::MYSQL)]
import mysql.connector
				[elseif (type=DBTYPE::POSTGRESQL)]
import psycopg2
				[elseif (type=DBTYPE::SQLSERVER)]
import pyodbc
				[elseif (type=DBTYPE::ORACLE)]
import oracledb
				[elseif (type=DBTYPE::MONGODB)]
import pymongo
from pandas import json_normalize
				[elseif (type=DBTYPE::DYNAMODB)]
import boto3
from pandas import json_normalize
				[/if]
			[/for]
		[/let]
	[/if]
[/let]

[for (st : Storage | aWorkflow.environment.storage)]
	[if (st.oclIsKindOf(LocalFolder))]
		[let f : Folder = st.oclAsType(Folder)]
			[for (fi : File | f.file)]
				[if (fi.type=FileType::CSV)]
[fi.dataDictionary.name.replaceAll('[(),\\s]+', '_')/]=pd.read_csv('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[if (st.storageName.size()>0)][st.storageName/]/[/if][if (fi.dataDictionary.fileName.trim().indexOf('.')>0)][fi.dataDictionary.fileName.trim()/][else][fi.dataDictionary.fileName.trim()/].csv[/if]', sep = '[fi.csv_sep/]')
[fi.dataDictionary.name.replaceAll('[(),\\s]+', '_')/].to_parquet('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[if (st.storageName.size()>0)][st.storageName/]/[/if][if (fi.dataDictionary.fileName.trim().indexOf('.')>0)][fi.dataDictionary.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][fi.dataDictionary.fileName.trim()/].parquet[/if]')

				[elseif (fi.type=FileType::EXCEL)]
[fi.dataDictionary.name.replaceAll('[(),\\s]+', '_')/]=pd.read_excel('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[if (st.storageName.size()>0)][st.storageName/]/[/if][if (fi.dataDictionary.fileName.trim().indexOf('.')>0)][fi.dataDictionary.fileName.trim()/][else][fi.dataDictionary.fileName.trim()/].xlsx[/if]')
[fi.dataDictionary.name.replaceAll('[(),\\s]+', '_')/].to_parquet('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[if (st.storageName.size()>0)][st.storageName/]/[/if][if (fi.dataDictionary.fileName.trim().indexOf('.')>0)][fi.dataDictionary.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][fi.dataDictionary.fileName.trim()/].parquet[/if]')

				[elseif (fi.type=FileType::FEATHER)]
[fi.dataDictionary.name.replaceAll('[(),\\s]+', '_')/]=pd.read_feather('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[if (st.storageName.size()>0)][st.storageName/]/[/if][if (fi.dataDictionary.fileName.trim().indexOf('.')>0)][fi.dataDictionary.fileName.trim()/][else][fi.dataDictionary.fileName.trim()/].feather[/if]')
[fi.dataDictionary.name.replaceAll('[(),\\s]+', '_')/].to_parquet('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[if (st.storageName.size()>0)][st.storageName/]/[/if][if (fi.dataDictionary.fileName.trim().indexOf('.')>0)][fi.dataDictionary.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][fi.dataDictionary.fileName.trim()/].parquet[/if]')

				[elseif (fi.type=FileType::HDF5)]
[if (fi.dataDictionary.fileName.trim().indexOf('.')>0)][fi.dataDictionary.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '')/][else][fi.dataDictionary.fileName.trim()/][/if]=pd.read_hdf('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[if (st.storageName.size()>0)][st.storageName/]/[/if][if (fi.dataDictionary.fileName.trim().indexOf('.')>0)][fi.dataDictionary.fileName.trim()/][else][fi.dataDictionary.fileName.trim()/].hdf5[/if]'[if (fi.hdf5_key<>'')], key='[fi.hdf5_key/]'[/if])
[if (fi.dataDictionary.fileName.trim().indexOf('.')>0)][fi.dataDictionary.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '')/][else][fi.dataDictionary.fileName.trim()/][/if].to_parquet('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[if (st.storageName.size()>0)][st.storageName/]/[/if][if (fi.dataDictionary.fileName.trim().indexOf('.')>0)][fi.dataDictionary.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][fi.dataDictionary.fileName.trim()/].parquet[/if]')

				[elseif (fi.type=FileType::JSON)]
[fi.dataDictionary.name.replaceAll('[(),\\s]+', '_')/]=pd.json_normalize('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[if (st.storageName.size()>0)][st.storageName/]/[/if][if (fi.dataDictionary.fileName.trim().indexOf('.')>0)][fi.dataDictionary.fileName.trim()/][else][fi.dataDictionary.fileName.trim()/].json[/if]')
[fi.dataDictionary.name.replaceAll('[(),\\s]+', '_')/].to_parquet('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[if (st.storageName.size()>0)][st.storageName/]/[/if][if (fi.dataDictionary.fileName.trim().indexOf('.')>0)][fi.dataDictionary.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][fi.dataDictionary.fileName.trim()/].parquet[/if]')

				[/if]
			[/for]
		[/let]
	[elseif (st.oclIsKindOf(Database))]
		[let db : Database = st.oclAsType(Database)]
			[if (db.type=DBTYPE::MYSQL)]
				[if (db.auth.oclIsKindOf(Credentials))]
					[let cr : Credentials = db.auth.oclAsType(Credentials)]
engine = create_engine('mysql+mysqlconnector://[cr.username/]:[cr.passwd/]@[db.host/]/[db.dbName/]')
					[/let]
				[elseif (db.auth.oclIsKindOf(OAuth2))]
					[let oa : OAuth2 = db.auth.oclAsType(OAuth2)]
					[/let]
				[elseif (db.auth.oclIsKindOf(SSL))]
					[let ssl : SSL = db.auth.oclAsType(SSL)]
					[/let]
				[/if]
				[for (tab : Table | db.table)]
[db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/] = pd.read_sql('SELECT * FROM [if (tab.dataDictionary.fileName.trim().indexOf('.')>0)][tab.dataDictionary.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '')/][else][tab.dataDictionary.fileName.trim()/][/if];', engine)
[db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/].to_parquet('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[if (st.storageName.size()>0)][st.storageName/]/[/if][if (tab.dataDictionary.fileName.trim().indexOf('.')>0)][tab.dataDictionary.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][tab.dataDictionary.fileName.trim()/].parquet[/if]')

				[/for]
			[elseif (db.type=DBTYPE::POSTGRESQL)]
				[if (db.auth.oclIsKindOf(Credentials))]
					[let cr : Credentials = db.auth.oclAsType(Credentials)]
engine = create_engine('postgresql+psycopg2://[cr.username/]:[cr.passwd/]@[db.host/]/[db.dbName/]')
					[/let]
				[elseif (db.auth.oclIsKindOf(OAuth2))]
					[let oa : OAuth2 = db.auth.oclAsType(OAuth2)]
					[/let]
				[elseif (db.auth.oclIsKindOf(SSL))]
					[let ssl : SSL = db.auth.oclAsType(SSL)]
					[/let]
				[/if]
				[for (tab : Table | db.table)]
[db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/] = pd.read_sql('SELECT * FROM [if (tab.dataDictionary.fileName.trim().indexOf('.')>0)][tab.dataDictionary.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '')/][else][tab.dataDictionary.fileName.trim()/][/if];', engine)
[db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/].to_parquet('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[if (st.storageName.size()>0)][st.storageName/]/[/if][if (tab.dataDictionary.fileName.trim().indexOf('.')>0)][tab.dataDictionary.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][tab.dataDictionary.fileName.trim()/].parquet[/if]')

				[/for]
			[elseif (db.type=DBTYPE::SQLSERVER)]
				[if (db.auth.oclIsKindOf(Credentials))]
					[let cr : Credentials = db.auth.oclAsType(Credentials)]
engine = create_engine('mssql+pyodbc://[cr.username/]:[cr.passwd/]@[db.host/]/[db.dbName/]?driver=ODBC+Driver+17+for+SQL+Server')
					[/let]
				[elseif (db.auth.oclIsKindOf(OAuth2))]
					[let oa : OAuth2 = db.auth.oclAsType(OAuth2)]
					[/let]
				[elseif (db.auth.oclIsKindOf(SSL))]
					[let ssl : SSL = db.auth.oclAsType(SSL)]
					[/let]
				[/if]
				[for (tab : Table | db.table)]
[db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/] = pd.read_sql('SELECT * FROM [if (tab.dataDictionary.fileName.trim().indexOf('.')>0)][tab.dataDictionary.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '')/][else][tab.dataDictionary.fileName.trim()/][/if];', engine)
[db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/].to_parquet('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[if (st.storageName.size()>0)][st.storageName/]/[/if][if (tab.dataDictionary.fileName.trim().indexOf('.')>0)][tab.dataDictionary.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][tab.dataDictionary.fileName.trim()/].parquet[/if]')

				[/for]
			[elseif (db.type=DBTYPE::ORACLE)]
				[if (db.auth.oclIsKindOf(Credentials))]
					[let cr : Credentials = db.auth.oclAsType(Credentials)]
engine = create_engine('oracle+oracledb://[cr.username/]:[cr.passwd/]@[db.host/]:[db.port/]/?service_name=[db.dbName/]')
					[/let]
				[elseif (db.auth.oclIsKindOf(OAuth2))]
					[let oa : OAuth2 = db.auth.oclAsType(OAuth2)]
					[/let]
				[elseif (db.auth.oclIsKindOf(SSL))]
					[let ssl : SSL = db.auth.oclAsType(SSL)]
					[/let]
				[/if]
				[for (tab : Table | db.table)]
[db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/] = pd.read_sql('SELECT * FROM [if (tab.dataDictionary.fileName.trim().indexOf('.')>0)][tab.dataDictionary.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '')/][else][tab.dataDictionary.fileName.trim()/][/if]', engine)
[db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/].to_parquet('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[if (st.storageName.size()>0)][st.storageName/]/[/if][if (tab.dataDictionary.fileName.trim().indexOf('.')>0)][tab.dataDictionary.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][tab.dataDictionary.fileName.trim()/].parquet[/if]')

				[/for]
			[elseif (db.type=DBTYPE::MONGODB)]
				[if (not db.auth->isEmpty())]
					[if (db.auth.oclIsKindOf(Credentials))]
						[let cr : Credentials = db.auth.oclAsType(Credentials)]
client = pymongo.MongoClient("mongodb://[cr.username/]:[cr.passwd/]@[db.host/]:[db.port/]/")
						[/let]
					[elseif (db.auth.oclIsKindOf(OAuth2))]
						[let oa : OAuth2 = db.auth.oclAsType(OAuth2)]
						[/let]
					[elseif (db.auth.oclIsKindOf(SSL))]
						[let ssl : SSL = db.auth.oclAsType(SSL)]
						[/let]
					[/if]
				[elseif (db.auth->isEmpty())]
client = pymongo.MongoClient("mongodb://[db.host/]:[db.port/]/")
				[/if]
[db.dbName/]_db = client['['/]"[db.dbName/]"[']'/]
				[for (tab : Table | db.table)]
[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/]_collection = [db.dbName/]_db['['/]"[if (tab.dataDictionary.fileName.trim().indexOf('.')>0)][tab.dataDictionary.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '')/][else][tab.dataDictionary.fileName.trim()/][/if]"[']'/]
[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/]_data = [tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/]_collection.find()
[db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/] = pd.DataFrame(json_normalize([tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/]_data))

if '_id' in [db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/].columns:
    [db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/]['['/]'_id'[']'/] = [db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/]['['/]'_id'[']'/].astype(str)

[db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/].to_parquet('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[if (st.storageName.size()>0)][st.storageName/]/[/if][if (tab.dataDictionary.fileName.trim().indexOf('.')>0)][tab.dataDictionary.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][tab.dataDictionary.fileName.trim()/].parquet[/if]')

				[/for]
			[elseif (db.type=DBTYPE::DYNAMODB)]
				[if (not db.auth->isEmpty())]
					[if (db.auth.oclIsKindOf(AWS))]
						[let aws : AWS = db.auth.oclAsType(AWS)]
dynamodb = boto3.resource(
    'dynamodb',
    region_name="[aws.region/]",
    aws_access_key_id='[aws.accessKey/]',
    aws_secret_access_key='[aws.secretId/]',
    endpoint_url="[aws.endpointURL/]"
)
						[/let]
					[/if]
				[/if]
				[for (tab : Table | db.table)]
[db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/]_table = dynamodb.Table('[if (tab.dataDictionary.fileName.trim().indexOf('.')>0)][tab.dataDictionary.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '')/][else][tab.dataDictionary.fileName.trim()/][/if]')
[db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/]_response = [db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/]_table.scan()
[db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/]_data = [db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/]_response['['/]'Items'[']'/]
[db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/] = pd.DataFrame(json_normalize([db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/]_data))
[db.dbName/]_[tab.dataDictionary.name.replaceAll('[(),\\s]+', '_')/].to_parquet('[if (aWorkflow.environment.path.startsWith('.'))][aWorkflow.environment.path.replaceFirst('./', '/')/][else][aWorkflow.environment.path/][/if]/data/[if (st.storageName.size()>0)][st.storageName/]/[/if][if (tab.dataDictionary.fileName.trim().indexOf('.')>0)][tab.dataDictionary.fileName.trim().replaceAll('\\.(.*?)(\\s|$)', '.parquet')/][else][tab.dataDictionary.fileName.trim()/].parquet[/if]')

				[/for]
			[/if]
		[/let]
	[/if]
[/for]
[/file]
[/template]


[template public pmmlProcessing(aWorkflow : Workflow)]
[file ('pmmlProcessing.py', false, 'UTF-8')]
import os
from enum import Enum
from pathlib import Path
import pandas as pd
import pypmml
import sklearn.metrics as sklearn_metrics
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
import xml.etree.ElementTree as ElementTreeXML


class ModelName(Enum):
    NAIVE_BAYES = "naive bayes"
    DECISION_TREE = "decision tree"
    LINEAR_REGRESSION = "linear regression"
    K_MEANS = "k-means"
    MLP = "multi-layer perceptron (Neural Network)"


class PMMLModel:
    """
    PMML model class to make predictions, save the predictions and metrics to a CSV file and train and
    validate the model

    Attributes:
    ----------
    inputDatasetFilePath: str
        Input dataset filepath
    outputDatasetFilePath: str
        Output dataset filepath
    pmml_model_filepath: str
        PMML model filepath
    inputDataset: pd.DataFrame
        Input dataset
    predictions: pd.DataFrame
        Predictions made by the model
    validation_predictions: pd.DataFrame
        Predictions to validate the model
    pmmlModelLearner: pypmml.Model
        PMML model learner
    exportOnlyPredictions: bool
        Export just the predictions or the whole dataset with predictions
    export_test_metrics: bool
        Export the test metrics or not
    algorithmName: str
        Algorithm type name
    modelName: ModelName
        Algorithm model name
    inputNames: list
        Input column names used by the model
    outputNames: list
        Output column names generated by the model
    predictedClassColumn: str
        Predicted class column name
    classColumn: str
        Class column name to predict
    metric_scores: pd.DataFrame
        Metric scores
    train_split: float
        Train split percentage for the dataset
    test_split: float
        Test split percentage for the dataset
    """
    inputDatasetFilePath: str = None  # Input dataset filepath
    outputDatasetFilePath: str = None  # Output dataset filepath
    pmml_model_filepath: str = None  # PMML model filepath

    inputDataset: pd.DataFrame = None  # Input dataset
    predictions: pd.DataFrame = None  # Predictions made by the model
    validation_predictions: pd.DataFrame = None  # Validation predictions

    pmmlModelLearner: pypmml.Model = None  # PMML model learner
    exportOnlyPredictions: bool = None  # Export just the predictions or the whole dataset with predictions
    export_test_metrics: bool = None  # Export the test metrics or not

    algorithmName: str = None  # Algorithm type name
    modelName: ModelName = None  # Algorithm model name

    inputNames: list = None  # Input column names used by the model
    outputNames: list = None  # Output column names generated by the model

    predictedClassColumn: str = None  # Predicted class column name
    classColumn: str = None  # Predicted class column name

    metric_scores: pd.DataFrame = None  # Metric scores

    train_split: float = None  # Train split percentage for the dataset
    test_split: float = None  # Test split percentage for the dataset

    def __init__(self, input_dataset_filepath: str, output_dataset_filepath: str, model_learner_pmml_filepath: str,
                 export_only_predictions: bool, export_test_metrics: bool, train_split: float = None,
                 test_split: float = None):
        """
        Initialize the PMML model object, parse the PMML model and make and export the predictions

        :param input_dataset_filepath: filepath to the input dataset to use to validate the model and make predictions
        :param output_dataset_filepath: filepath to the output dataset to save the predictions and metrics
        :param model_learner_pmml_filepath: filepath to the PMML model file
        :param export_only_predictions: export just the predictions or the whole dataset with predictions
        :param export_test_metrics: boolean to export the test metrics or not
        :param train_split: train split percentage for the dataset
        :param test_split: test split percentage for the dataset
        """
        self.inputDatasetFilePath = input_dataset_filepath
        self.outputDatasetFilePath = output_dataset_filepath
        self.pmml_model_filepath = model_learner_pmml_filepath

        self.inputDataset = pd.read_csv(self.inputDatasetFilePath)  # Load the input dataset as a dataframe
        self.pmmlModelLearner = pypmml.Model.load(model_learner_pmml_filepath)  # Load the PMML model
        self.exportOnlyPredictions = export_only_predictions  # Export just the predictions or the whole
        # dataset with predictions
        self.export_test_metrics = export_test_metrics  # Export the test metrics or not
        # Save the train and test split if they are not None
        if train_split is not None and test_split is not None:
            self.train_split = train_split
            self.test_split = test_split

        # MAIN FUNCTION CALLS: PARSE PMML MODEL, MAKE AND EXPORT PREDICTIONS AND SAVE TEST METRICS
        self.parser_pmml_model()  # Parse the PMML model
        self.make_and_export_predictions()  # Make and export the predictions
        self.save_test_metrics()  # Save the test metrics
        print(f"{self.modelName.value} model has been loaded\n")

    def get_model_name(self):
        """
        Get a uniform model name from the PMML model name
        """
        if self.pmmlModelLearner.modelName is not None:
            pmml_model_name = self.pmmlModelLearner.modelName
        else:
            pmml_model_name = self.pmmlModelLearner.modelElement
        model_name_lower = pmml_model_name.lower()
        best_match = None
        highest_score = 0

        # Calculate the score based on substring matches with variable lengths
        for name in ModelName:
            name_value = name.value.lower()
            score = 0

            # Iterate through all possible substrings of name_value
            for i in range(len(name_value)):
                for j in range(i + 1, len(name_value) + 1):
                    substring = name_value['['/]i:j[']'/]
                    if substring in model_name_lower:
                        score += 1

            # Save the best match
            if score > highest_score:
                highest_score = score
                best_match = name

        self.modelName = best_match

    def parser_pmml_model(self):
        """
        Parse the PMML model
        """
        self.algorithmName = self.pmmlModelLearner.functionName  # Get the function name
        self.get_model_name()
        self.inputNames = self.pmmlModelLearner.inputNames
        self.outputNames = self.pmmlModelLearner.outputNames

        # Get the predicted class column name
        if self.algorithmName != 'clustering':
            self.predictedClassColumn = self.outputNames['['/]0[']'/]  # Get the predicted class column name
            self.classColumn = self.predictedClassColumn.replace('predicted_', '')  # Get class column name

    def make_and_export_predictions(self):
        """
        Make and export the predictions
        """
        print("Making predictions on the dataset...")
        self.predictions = self.pmmlModelLearner.predict(self.inputDataset)  # Make predictions on the dataset

        # Create the output directory if it does not exist
        if not os.path.exists(self.outputDatasetFilePath):
            os.makedirs(self.outputDatasetFilePath)

        if not self.exportOnlyPredictions:
            predictions_df = self.inputDataset.copy()
            predictions_df = pd.concat(['['/]predictions_df, self.predictions[']'/], axis=1)
            predictions_df.to_csv(f'{self.outputDatasetFilePath}/{Path(self.inputDatasetFilePath).stem}'
                                  f'_with_predictions_using_{self.modelName.name}.csv', index=False)
            print("Predictions and dataset saved to a CSV file")
        else:
            self.predictions.to_csv(f'{self.outputDatasetFilePath}/{Path(self.inputDatasetFilePath).stem}'
                                    f'_only_predictions_using_{self.modelName.name}.csv', index=False)
            print("Only predictions saved to a CSV file")

    def save_test_metrics(self):
        """
        Save the test metrics to the PMMLModel class instance and export them to a CSV file if the attribute
        'export_test_metrics' is True
        """
        print("Testing model and saving metric results...")
        if self.algorithmName == 'classification':

            y_true = self.inputDataset['['/]self.classColumn[']'/]
            y_pred = self.predictions['['/]self.predictedClassColumn[']'/]
            y_true = pd.Categorical(y_true).codes
            y_pred = pd.Categorical(y_pred).codes

            accuracy = sklearn_metrics.accuracy_score(y_true, y_pred)
            precision = sklearn_metrics.precision_score(y_true, y_pred)
            recall = sklearn_metrics.recall_score(y_true, y_pred)
            f1 = sklearn_metrics.f1_score(y_true, y_pred)
            # Save the metrics to the PMMLModel object
            self.metric_scores = pd.DataFrame({
                'Metric': ['['/]'Accuracy', 'Precision', 'Recall', 'F1 Score'[']'/],
                'Score': ['['/]accuracy, precision, recall, f1[']'/]
            })

        elif self.algorithmName == 'regression':

            y_true = self.inputDataset['['/]self.classColumn[']'/]
            y_pred = self.predictions['['/]self.predictedClassColumn[']'/]
            mse = sklearn_metrics.mean_squared_error(y_true, y_pred)
            r2 = sklearn_metrics.r2_score(y_true, y_pred)
            mae = sklearn_metrics.mean_absolute_error(y_true, y_pred)
            median_ae = sklearn_metrics.median_absolute_error(y_true, y_pred)

            self.metric_scores = pd.DataFrame({
                'Metric': ['['/]'Mean Squared Error', 'R2 Score', 'Mean Absolute Error', 'Median Absolute Error'[']'/],
                'Score': ['['/]mse, r2, mae, median_ae[']'/]
            })

        elif self.algorithmName == 'clustering':
            cluster_names = self.predictions['['/]'cluster_name'[']'/]

            # Count the number of instances in each cluster
            cluster_counts = cluster_names.value_counts()

            self.metric_scores = pd.DataFrame({
                'Metric': ['['/]'Cluster Counts'[']'/],
                'Score': ['['/]cluster_counts[']'/]
            })

        else:
            raise ValueError(f"{self.modelName.value} model not recognized")

        if self.export_test_metrics:
            if not os.path.exists(self.outputDatasetFilePath):
                os.makedirs(self.outputDatasetFilePath)

            self.metric_scores.to_csv(f'{self.outputDatasetFilePath}/{Path(self.inputDatasetFilePath).stem}'
                                      f'_metric_scores_using_{self.modelName.name}.csv', index=False)
            print("Metric scores saved to a CSV file")

    def train_and_validate_model(self):
        """
        Train and validate the model if the attribute 'validate_model' is True
        """
        print(f"Training and validating the {self.modelName.value} model...")

        model_validated = False

        # Split the dataset into train and test using train_test_split from sklearn
        x = self.inputDataset['['/]self.inputNames[']'/]
        y = self.inputDataset['['/]self.classColumn[']'/] if self.classColumn is not None else None
        y_test = None
        y_train = None
        if self.classColumn is not None:
            x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=self.train_split,
                                                                test_size=self.test_split, shuffle=True)
        else:
            x_train, x_test = train_test_split(x, train_size=self.train_split, test_size=self.test_split, shuffle=True)

        # Prepare the training data to convert categorical columns to numerical columns
        x_train = x_train.apply(lambda col: pd.Categorical(col).codes)
        x_test = x_test.apply(lambda col: pd.Categorical(col).codes)

        validation_model = None
        # Train the model using the training dataset
        if self.modelName == ModelName.K_MEANS:
            # Carga el archivo PMML
            tree = ElementTreeXML.parse(self.pmml_model_filepath)
            root = tree.getroot()

            # Encuentra el ClusteringModel y accede a 'numberOfClusters'
            clustering_model = root.find('.//{http://www.dmg.org/PMML-4_2}'+ self.pmmlModelLearner.modelElement)
            if clustering_model is not None:
                number_of_clusters = int(clustering_model.get('numberOfClusters'))
                validation_model = KMeans(n_clusters=number_of_clusters)
                validation_model.fit(x_train)
            else:
                print("No se encontr el modelo de clustering.")

        elif self.modelName == ModelName.LINEAR_REGRESSION:
            validation_model = LinearRegression()
            validation_model.fit(x_train, y_train)
        elif self.modelName == ModelName.DECISION_TREE:
            validation_model = DecisionTreeClassifier()
            validation_model.fit(x_train, y_train)
        elif self.modelName == ModelName.NAIVE_BAYES:
            validation_model = GaussianNB()
            validation_model.fit(x_train, y_train)
        elif self.modelName == ModelName.MLP:
            validation_model = MLPClassifier()
            validation_model.fit(x_train, y_train)
        else:
            raise ValueError(f"{self.modelName.value} model not recognized")

        print(f"{self.modelName.value} model has been TRAINED")

        # Make validation predictions
        self.validation_predictions = validation_model.predict(x_test)

        # Validate the model
        if self.algorithmName == 'classification':

            y_true = y_test
            y_pred = self.validation_predictions
            y_true = pd.Categorical(y_true).codes
            y_pred = pd.Categorical(y_pred).codes

            accuracy = sklearn_metrics.accuracy_score(y_true, y_pred)
            precision = sklearn_metrics.precision_score(y_true, y_pred)
            recall = sklearn_metrics.recall_score(y_true, y_pred)
            f1 = sklearn_metrics.f1_score(y_true, y_pred)

            original_accuracy = self.metric_scores.loc['['/]self.metric_scores['['/]'Metric'[']'/] == 'Accuracy', 'Score'[']'/].values['['/]0[']'/]
            original_precision = self.metric_scores.loc['['/]self.metric_scores['['/]'Metric'[']'/] == 'Precision', 'Score'[']'/].values['['/]0[']'/]
            original_recall = self.metric_scores.loc['['/]self.metric_scores['['/]'Metric'[']'/] == 'Recall', 'Score'[']'/].values['['/]0[']'/]
            original_f1 = self.metric_scores.loc['['/]self.metric_scores['['/]'Metric'[']'/] == 'F1 Score', 'Score'[']'/].values['['/]0[']'/]

            print(f"Pretrained Model Accuracy: {original_accuracy} - Validation Model Accuracy: {accuracy}")
            print(f"Pretrained Model Precision: {original_precision} - Validation Model Precision: {precision}")
            print(f"Pretrained Model Recall: {original_recall} - Validation Model Recall: {recall}")
            print(f"Pretrained Model F1 Score: {original_f1} - Validation Model F1 Score: {f1}")

        elif self.algorithmName == 'regression':

            y_true = y_test
            y_pred = self.validation_predictions
            mse = sklearn_metrics.mean_squared_error(y_true, y_pred)
            r2 = sklearn_metrics.r2_score(y_true, y_pred)
            mae = sklearn_metrics.mean_absolute_error(y_true, y_pred)
            median_ae = sklearn_metrics.median_absolute_error(y_true, y_pred)

            original_mse = self.metric_scores.loc['['/]self.metric_scores['['/]'Metric'[']'/] == 'Mean Squared Error', 'Score'[']'/].values['['/]0[']'/]
            original_r2 = self.metric_scores.loc['['/]self.metric_scores['['/]'Metric'[']'/] == 'R2 Score', 'Score'[']'/].values['['/]0[']'/]
            original_mae = self.metric_scores.loc['['/]self.metric_scores['['/]'Metric'[']'/] == 'Mean Absolute Error', 'Score'[']'/].values['['/]0[']'/]
            original_median_ae = self.metric_scores.loc['['/]self.metric_scores['['/]'Metric'[']'/] == 'Median Absolute Error', 'Score'[']'/].values['['/]0[']'/]

            print(f"Pretrained Model Mean Squared Error: {original_mse} - Validation Model Mean Squared Error: {mse}")
            print(f"Pretrained Model R2 Score: {original_r2} - Validation Model R2 Score: {r2}")
            print(f"Pretrained Model Mean Absolute Error: {original_mae} - Validation Model Mean Absolute Error: {mae}")
            print(f"Pretrained Model Median Absolute Error: {original_median_ae} - Validation Model Median Absolute Error: {median_ae}")

        elif self.algorithmName == 'clustering':

            # Count the number of instances in each cluster and compare with the original cluster counts
            cluster_counts = pd.Series(validation_model.labels_).value_counts()
            original_cluster_counts = self.metric_scores.loc['['/]self.metric_scores['['/]'Metric'[']'/] == 'Cluster Counts', 'Score'[']'/].values['['/]0[']'/]
            print(f"Pretrained Model Cluster Counts: {original_cluster_counts} - Validation Model Cluster Counts: {cluster_counts}")

            # Check if the cluster counts are the same considering an epsilon error
            if (original_cluster_counts - cluster_counts).abs().max() < 1e-5:
                print("Cluster counts are the same")
                model_validated = True
            else:
                print("Cluster counts are NOT the same")
        else:
            raise ValueError(f"{self.modelName.value} model not recognized")

        if model_validated:
            print(f"{self.modelName.value} model has been VALIDATED\n")
        else:
            print(f"{self.modelName.value} model has NOT BEEN VALIDATED\n")

    def __str__(self):
        """
        String representation of the PMML model
        :return: string representation of the PMML model
        """
        return f"Algorithm type name: {self.algorithmName}\n" \
               f"Model name: {self.modelName.value}\n" \
               f"Input names: {self.inputNames}\n" \
               f"Output names: {self.outputNames}\n"
[/file]
[/template]




